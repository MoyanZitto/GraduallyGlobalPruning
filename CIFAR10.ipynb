{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# load general module\n",
    "from __future__ import print_function\n",
    "from keras.optimizers import SGD\n",
    "import numpy as np\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "import h5py\n",
    "from keras.layers import Dense, Convolution2D, BatchNormalization, MaxPooling2D, Input, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.utils.layer_utils import layer_from_config\n",
    "import keras.backend as K\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# load data, prepare for training\n",
    "\n",
    "# settings\n",
    "batch_size = 32\n",
    "nb_classes = 10\n",
    "nb_epoch = 200\n",
    "data_augmentation = True\n",
    "trained = False\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 32, 32\n",
    "# the CIFAR10 images are RGB\n",
    "img_channels = 3\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255.\n",
    "X_test /= 255.\n",
    "X_train -= np.mean(X_train, axis=0)\n",
    "X_test -= np.mean(X_test, axis=0)\n",
    "\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Jan 25 10:43:17 2017\n",
    "\n",
    "@author: moyan\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import h5py\n",
    "from keras.models import Sequential, Model\n",
    "import keras.backend as K\n",
    "import copy\n",
    "from keras.layers import Dense, Input, Flatten, BatchNormalization, Convolution2D, MaxPooling2D\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_network():\n",
    "    \"\"\"\n",
    "    return a vgg-like cnn for cifar10 classification, not trained.\n",
    "    \"\"\"\n",
    "    img_input = Input(shape=(32,32,3))\n",
    "    x = Convolution2D(64, 3, 3, activation='relu', border_mode= 'same', name='block1_conv1')(img_input)\n",
    "    x = BatchNormalization(axis=3, name='block1_bn1')(x)\n",
    "    x = Convolution2D(64, 3, 3, activation='relu', border_mode='same', name='block1_conv2')(x)\n",
    "    x = BatchNormalization(axis=3, name='block1_bn2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = Convolution2D(128, 3, 3, activation='relu', border_mode='same', name='block2_conv1')(x)\n",
    "    x = BatchNormalization(axis=3, name='block2_bn1')(x)\n",
    "    x = Convolution2D(128, 3, 3, activation='relu', border_mode='same', name='block2_conv2')(x)\n",
    "    x = BatchNormalization(axis=3, name='block2_bn2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = Convolution2D(256, 3, 3, activation='relu', border_mode='same', name='block3_conv1')(x)\n",
    "    x = BatchNormalization(axis=3, name='block3_bn1')(x)\n",
    "    x = Convolution2D(256, 3, 3, activation='relu', border_mode='same', name='block3_conv2')(x)\n",
    "    x = BatchNormalization(axis=3, name='block3_bn2')(x)\n",
    "    x = Convolution2D(256, 3, 3, activation='relu', border_mode='same', name='block3_conv3')(x)\n",
    "    x = BatchNormalization(axis=3, name='block3_bn3')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block4_conv1')(x)\n",
    "    x = BatchNormalization(axis=3, name='block4_bn1')(x)\n",
    "    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block4_conv2')(x)\n",
    "    x = BatchNormalization(axis=3, name='block4_bn2')(x)\n",
    "    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block4_conv3')(x)\n",
    "    x = BatchNormalization(axis=3, name='block4_bn3')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "    # Block 5\n",
    "    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block5_conv1')(x)\n",
    "    x = BatchNormalization(axis=3, name='block5_bn1')(x)\n",
    "    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block5_conv2')(x)\n",
    "    x = BatchNormalization(axis=3, name='block5_bn2')(x)\n",
    "    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block5_conv3')(x)\n",
    "    x = BatchNormalization(axis=3, name='block5_bn3')(x)\n",
    "\n",
    "    x = Flatten(name='flatten')(x)\n",
    "    x = Dense(512, activation='relu', name='fc1')(x)\n",
    "    x = BatchNormalization(axis=1, name='fc_bn1')(x)\n",
    "    x = Dense(10, activation='softmax', name='predictions')(x)\n",
    "\n",
    "    model = Model([img_input], [x])\n",
    "    return model\n",
    "\n",
    "\n",
    "def MoveNetwork(net, layer_before_flatten=None):\n",
    "    \"\"\"\n",
    "    This function will extract a subnetwork from the original network, only work with plain networks.\n",
    "    A plain network is a network with only one input and one output, contains only:\n",
    "        - Convolution2D\n",
    "        - MaxPooling2D/AveragePooling2D\n",
    "        - BatchNormalization\n",
    "        - Dropout\n",
    "        - Flatten\n",
    "        - Dense\n",
    "        - Activation\n",
    "    Neurons whose parameters contain at least a non-zero value would be extracted.\n",
    "\n",
    "    params:\n",
    "        - net: A Keras model\n",
    "        - layer_before_flatten: str, the name of the convolution layer just before the flatten layer.\n",
    "    returns:\n",
    "        - A sub-network extracted from the original one.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    weight_list = []\n",
    "    for layer in net.layers:\n",
    "        if layer.__class__.__name__ == \"InputLayer\":\n",
    "            continue\n",
    "        if layer.__class__.__name__ == 'Convolution2D':\n",
    "            if len(model.layers)==0:\n",
    "                w,b = layer.get_weights()\n",
    "                save_id = []\n",
    "                for fid in range(w.shape[3]):\n",
    "                    if np.sum(np.abs(w[:,:,:,fid]))!=0.:\n",
    "                        save_id.append(fid)\n",
    "                w_new = np.zeros(w.shape[:3]+(len(save_id),))\n",
    "                b_new = np.zeros(len(save_id))\n",
    "\n",
    "                i = 0\n",
    "                for fid in save_id:\n",
    "                    w_new[:,:,:,i] = w[:,:,:,fid]\n",
    "                    b_new[i] = b[fid]\n",
    "                    i+=1\n",
    "                config = layer.get_config()\n",
    "                new_layer = Convolution2D(len(save_id), config['nb_col'], config['nb_row'],\n",
    "                                          activation=config['activation'], border_mode=config['border_mode'],\n",
    "                                          name=config['name'], input_shape=(32,32,3))\n",
    "                weight_list.append([w_new, b_new])\n",
    "                model.add(new_layer)\n",
    "                prev_save_id = copy.deepcopy(save_id)\n",
    "            else:\n",
    "                nb_channels = len(prev_save_id)\n",
    "                w,b = layer.get_weights()\n",
    "                save_id = []\n",
    "                for fid in range(w.shape[3]):\n",
    "                    if np.sum(np.abs(w[:,:,:,fid]))>0.0000001:\n",
    "                        save_id.append(fid)\n",
    "\n",
    "                w_new = np.zeros(w.shape[:2]+(nb_channels,len(save_id)))\n",
    "                b_new = np.zeros(len(save_id))\n",
    "\n",
    "                i = 0\n",
    "                for fid in save_id:\n",
    "                    w_new[:,:,:,i] = w[:,:,prev_save_id,fid]\n",
    "                    b_new[i] = b[fid]\n",
    "                    i+=1\n",
    "                config = layer.get_config()\n",
    "                new_layer = Convolution2D(len(save_id), config['nb_col'], config['nb_row'],\n",
    "                          activation=config['activation'], border_mode=config['border_mode'],\n",
    "                          name=config['name'])\n",
    "                weight_list.append([w_new, b_new])\n",
    "                model.add(new_layer)\n",
    "                prev_save_id = copy.deepcopy(save_id)\n",
    "\n",
    "        elif layer.__class__.__name__ == 'BatchNormalization':\n",
    "            gamma, beta, mean, std = layer.get_weights()\n",
    "            nb_channels = len(prev_save_id)\n",
    "            gamma_new = np.zeros(len(prev_save_id))\n",
    "            beta_new = np.zeros(len(prev_save_id))\n",
    "            mean_new = np.zeros(len(prev_save_id))\n",
    "            std_new = np.zeros(len(prev_save_id))\n",
    "\n",
    "\n",
    "            gamma_new = gamma[prev_save_id]\n",
    "            beta_new = beta[prev_save_id]\n",
    "            mean_new = mean[prev_save_id]\n",
    "            std_new = std[prev_save_id]\n",
    "\n",
    "\n",
    "            config = layer.get_config()\n",
    "            new_layer = BatchNormalization(name=config['name'], axis=config['axis'])\n",
    "            weight_list.append([gamma_new, beta_new, mean_new, std_new])\n",
    "            model.add(new_layer)\n",
    "            \n",
    "        elif layer.__class__.__name__ == 'MaxPooling2D' or layer.__class__.__name__ == 'AveragePooling2D':\n",
    "            config = layer.get_config()\n",
    "            new_layer = MaxPooling2D(pool_size = config['pool_size'], strides = config['pool_size'], name = config['name'])\n",
    "            model.add(new_layer)\n",
    "            weight_list.append(None)\n",
    "\n",
    "        elif layer.__class__.__name__=='Flatten':\n",
    "            new_layer = Flatten(name = 'flatten')\n",
    "            model.add(new_layer)\n",
    "            weight_list.append(None)\n",
    "\n",
    "        elif layer.__class__.__name__=='Dense':\n",
    "            if model.layers[-1].__class__.__name__=='Flatten':\n",
    "                last_layer = net.get_layer(layer_before_flatten)\n",
    "                last_layer_output_shape = K.int_shape(last_layer.output)[1:]\n",
    "                fake_input = np.zeros(last_layer_output_shape)\n",
    "                fake_input[:,:,prev_save_id] = 1.\n",
    "                fake_input = np.expand_dims(fake_input, axis=0)\n",
    "\n",
    "                input_tensor = K.placeholder(fake_input.shape)\n",
    "                output_tensor = Flatten()(input_tensor)\n",
    "                get_indicator = K.function([input_tensor, K.learning_phase()],\n",
    "                          [output_tensor])\n",
    "                indicators = get_indicator([fake_input, 0])[0]\n",
    "                indicators = np.squeeze(indicators)\n",
    "                indicator_index = []\n",
    "                for i in range(indicators.shape[0]):\n",
    "                    if indicators[i]==1.:\n",
    "                        indicator_index.append(i)\n",
    "                save_id = []\n",
    "                w,b = layer.get_weights()\n",
    "                for i in range(w.shape[1]):\n",
    "                    if np.sum(np.abs(w[:,i]))!=0.:\n",
    "                        save_id.append(i)\n",
    "                w_new = w[indicator_index,:]\n",
    "                w_new = w_new[:,save_id]\n",
    "                b_new = b[save_id]\n",
    "                config = layer.get_config()\n",
    "                new_layer = Dense(len(save_id), activation=config['activation'], name=config['name'])\n",
    "                weight_list.append([w_new,b_new])\n",
    "                model.add(new_layer)\n",
    "                prev_save_id = copy.deepcopy(save_id)\n",
    "\n",
    "            else:\n",
    "                w,b = layer.get_weights()\n",
    "                w_new = w[prev_save_id,:]\n",
    "                config = layer.get_config()\n",
    "                new_layer = Dense(config['output_dim'], activation=config['activation'], name=config['name'])\n",
    "                weight_list.append([w_new,b])\n",
    "                model.add(new_layer)\n",
    "\n",
    "        elif layer.__class__.__name__=='Dropout':\n",
    "            config = layer.get_config()\n",
    "            new_layer = Dropout(config['p'])\n",
    "            weight_list.append(None)\n",
    "            model.add(new_layer)\n",
    "\n",
    "            \n",
    "        else:\n",
    "            print(\"Exist unknown layer type:%s\"%layer.name)\n",
    "\n",
    "\n",
    "    for i in range(len(model.layers)):\n",
    "        if weight_list[i] == None:\n",
    "            continue\n",
    "        model.layers[i].set_weights(weight_list[i])\n",
    "    return model\n",
    "\n",
    "\n",
    "def next_layer(target_layer):\n",
    "    \"\"\"\n",
    "    This function is set to find the BatchNormalization after the conv layers and fc layers\n",
    "    you may always need to define this function if your network contains BatchNormalization layer\n",
    "\n",
    "    \"\"\"\n",
    "    next_layers={\"block1_conv1\":\"block1_bn1\",\n",
    "               \"block1_conv2\":\"block1_bn2\",\n",
    "               \"block2_conv1\":\"block2_bn1\",\n",
    "               \"block2_conv2\":\"block2_bn2\",\n",
    "               \"block3_conv1\":\"block3_bn1\",\n",
    "               \"block3_conv2\":\"block3_bn2\",\n",
    "               \"block3_conv3\":\"block3_bn3\",\n",
    "               \"block4_conv1\":\"block4_bn1\",\n",
    "               \"block4_conv2\":\"block4_bn2\",\n",
    "               \"block4_conv3\":\"block4_bn3\",\n",
    "               \"block5_conv1\":\"block5_bn1\",\n",
    "               \"block5_conv2\":\"block5_bn2\",\n",
    "               \"block5_conv3\":\"block5_bn3\",\n",
    "               \"fc1\":\"fc_bn1\"\n",
    "          }\n",
    "    return next_layers[target_layer]\n",
    "\n",
    "\n",
    "\n",
    "def get_activation(model, prefix, X_train, y_train):\n",
    "    \"\"\"\n",
    "    This function obtain the activation of each layer when using data-dependent neuron selection strategy and\n",
    "    save them in several hdf5 files.\n",
    "    params:\n",
    "        model: the network\n",
    "        prefix: the name\n",
    "    \"\"\"\n",
    "\n",
    "    for target_layer in model.layers:\n",
    "        if target_layer.__class__.__name__ in [\"Convolution2D\", \"Dense\"]:\n",
    "            input_tensor = model.input\n",
    "            output_tensor = target_layer.output\n",
    "\n",
    "            get_outputs = K.function([input_tensor, K.learning_phase()],\n",
    "                              [output_tensor])\n",
    "\n",
    "            raw_stat = {}\n",
    "            for s in range(X_train.shape[0]):\n",
    "                img = np.expand_dims(X_train[s], axis=0)\n",
    "                label = y_train[s]\n",
    "                output = get_outputs([img, 0])[0]\n",
    "                output = np.squeeze(output)\n",
    "                if target_layer.__class__.__name__=='Dense':\n",
    "                    output = output\n",
    "                elif target_layer.__class__.__name__=='Convolution2D':\n",
    "                    output = np.mean(output, axis = (0,1))\n",
    "                assert(len(output.shape)==1)\n",
    "\n",
    "                if raw_stat.has_key(int(label)):\n",
    "                    raw_stat[int(label)].append(output)\n",
    "                else:\n",
    "                    raw_stat[int(label)]= [output]\n",
    "\n",
    "\n",
    "            f = h5py.File(save_name,'w')\n",
    "            for k in raw_stat.keys():\n",
    "                f.create_dataset(name=str(k), data=raw_stat[k])\n",
    "            f.close()\n",
    "\n",
    "\n",
    "class NetScore(object):\n",
    "    \"\"\"\n",
    "    This class record the contribution score of all neurons in network. And provide a series of\n",
    "    methods to get the redundant neurons.\n",
    "\n",
    "    You may set some properties when instantiating.\n",
    "\n",
    "    protect_dict: dict, mapping layer_name -> nb_neurons to be protected. If given, at least 'protect_dict[layer_name]'' neurons\n",
    "                  at 'layer_name' would be preserved.\n",
    "    model: str, \"global\" or \"layer_wise\", default \"global\". If set to \"layer_wise\", the number of redundant neurons in each layer\n",
    "                will be determined by a same pruning ratio or the number given by a dict.\n",
    "                if set to \"global\", the number of redundant neurons in each layer will be determined according to the score.\n",
    "\n",
    "    The methods are:\n",
    "\n",
    "        add(layer_name, neuron_id, score): add an neuron in the neuron list.\n",
    "            params:\n",
    "                layer_name: str, the name of layer that contains the neuron\n",
    "                neuron_id: int, the index of the neuron in this layer\n",
    "                score: float, the contribution score of the layer\n",
    "\n",
    "        sort(): sort the neurons according the contribution score.\n",
    "        get_redundant_neurons(p): return redundant neurons\n",
    "            params:\n",
    "                p: float or dict, if dict, the mode must be layer_wise. if float(0.0~1.0), in layer_wise mode, the redundant neurons\n",
    "                   in each layer will be selected according to this parameter. In \"global\" mode, this is the overall pruning ratio\n",
    "                   of the whole network.\n",
    "        print_info(): print infos.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, protect_dict=None, mode=\"global\"):\n",
    "        self.netscores = {}\n",
    "        self.sorted = False\n",
    "        self.total_num = 0\n",
    "        self.protected_num = 0\n",
    "        self.neuron_num = {}\n",
    "        self.prune_num = {}\n",
    "        self.protect_dict = protect_dict\n",
    "        self.mode = mode\n",
    "        self.redundant = None\n",
    "\n",
    "    def add(self, layer_name, neuron_id, score):\n",
    "        if self.netscores.has_key(layer_name):\n",
    "            self.netscores[layer_name].append((layer_name, neuron_id, score))\n",
    "            self.neuron_num[layer_name] += 1\n",
    "        else:\n",
    "            self.netscores[layer_name] = [(layer_name, neuron_id, score)]\n",
    "            self.neuron_num[layer_name] = 1\n",
    "\n",
    "        self.total_num += 1\n",
    "\n",
    "    def sort(self):\n",
    "        if self.mode == \"layer_wise\":\n",
    "            for k in self.netscores.keys():\n",
    "                self.netscores[k] = sorted(self.netscores[k], key=lambda x:x[2])\n",
    "            self.sorted = True\n",
    "        elif self.mode == \"global\":\n",
    "            netscores = []\n",
    "            if self.protect_dict != None:\n",
    "                for k in self.netscores.keys():\n",
    "                    self.netscores[k] = sorted(self.netscores[k], key=lambda x:x[2])\n",
    "                    self.netscores[k] = self.netscores[k][:-self.protect_dict[k]]\n",
    "            for k in self.netscores.keys():\n",
    "                netscores += self.netscores[k]\n",
    "            self.netscores = sorted(netscores, key=lambda x:x[2])\n",
    "            self.protected_num = self.total_num - len(self.netscores)\n",
    "\n",
    "    def get_redundant_neurons(self, p):\n",
    "        if isinstance(p, float):\n",
    "            if self.mode == \"layer_wise\":\n",
    "                if not self.sorted:\n",
    "                    self.sort()\n",
    "                redundant = []\n",
    "                print(type(self.netscores))\n",
    "                print(type(self.netscores.keys()))\n",
    "                for k in self.netscores.keys():\n",
    "                    redundant += self.netscores[k][:np.round(self.neuron_num[k]*p)]\n",
    "\n",
    "                self.redundant = redundant\n",
    "            elif self.mode == \"global\":\n",
    "                self.redundant = self.netscores[:int(np.round(self.total_num*p))]\n",
    "\n",
    "        if isinstance(p, dict):\n",
    "            if self.mode == \"layer_wise\":\n",
    "                if not self.sorted:\n",
    "                    self.sort()\n",
    "                redundant = []\n",
    "\n",
    "                for k in self.netscores.keys():\n",
    "                    redundant += self.netscores[k][:p[k]]\n",
    "                self.redundant = redundant\n",
    "            elif self.mode == \"global\":\n",
    "                raise TypeError(\"dict only work for global mode!\")\n",
    "                    \n",
    "\n",
    "        return self.redundant\n",
    "\n",
    "    def print_info(self):\n",
    "        for neuron in self.redundant:\n",
    "            layer_name, neuron_id, score = neuron\n",
    "            if self.prune_num.has_key(layer_name):\n",
    "                self.prune_num[layer_name] += 1\n",
    "            else:\n",
    "                self.prune_num[layer_name] = 1\n",
    "        if self.protect_dict == None:\n",
    "            for k in self.prune_num.keys():\n",
    "                print(\"%s: pruned:%d, protected:%d\"%(k, self.prune_num[k], 0))\n",
    "        else:\n",
    "            for k in self.prune_num.keys():\n",
    "                print(\"%s: pruned:%d, protected:%d\"%(k, self.prune_num[k], self.protect_dict[k]))\n",
    "        \n",
    "        print(\"pruning: %d -> %d, %d get pruned\"%(self.total_num, self.total_num-len(self.redundant), len(self.redundant)))\n",
    "\n",
    "\n",
    "def get_num_by_ratio(neuron_num,p):\n",
    "    \"\"\"\n",
    "    this function returns the dictionary according to pruning ratio.\n",
    "    this function should use with mode=\"layer_wise\" and the case when p is a dict.\n",
    "    We use np.round to erase the errors introduced by integer division\n",
    "    \"\"\"\n",
    "    total_num = 0\n",
    "    for k in neuron_num:\n",
    "        total_num += neuron_num[k]\n",
    "    total_pruned_num = np.round(total_num*p)\n",
    "    for k in neuron_num:\n",
    "        neuron_num[k] = int(np.round(total_pruned_num*(neuron_num[k]/float(total_num))))\n",
    "    return neuron_num\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1: pruned:230, protected:0\n",
      "block1_conv1: pruned:6, protected:0\n",
      "block5_conv2: pruned:1, protected:0\n",
      "pruning: 4736 -> 4499, 237 get pruned\n",
      "Moved Model ACC: 0.873800\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 197s - loss: 0.0045 - acc: 0.9986 - val_loss: 0.8405 - val_acc: 0.8687\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 189s - loss: 0.0026 - acc: 0.9991 - val_loss: 0.7868 - val_acc: 0.8740\n",
      "finetuned_acc: 0.874000\n",
      "round:1 nb_neurons:4736 acc:0.873800 fintune_acc:0.874000\n",
      "\n",
      "------------------------------\n",
      "fc1: pruned:140, protected:0\n",
      "block4_conv3: pruned:18, protected:0\n",
      "block5_conv3: pruned:16, protected:0\n",
      "block5_conv2: pruned:33, protected:0\n",
      "block5_conv1: pruned:17, protected:0\n",
      "block1_conv1: pruned:1, protected:0\n",
      "pruning: 4499 -> 4274, 225 get pruned\n",
      "Moved Model ACC: 0.873900\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 182s - loss: 0.0029 - acc: 0.9991 - val_loss: 0.9229 - val_acc: 0.8637\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 174s - loss: 0.0024 - acc: 0.9992 - val_loss: 0.8573 - val_acc: 0.8688\n",
      "finetuned_acc: 0.868800\n",
      "round:2 nb_neurons:4499 acc:0.873900 fintune_acc:0.868800\n",
      "\n",
      "------------------------------\n",
      "fc1: pruned:46, protected:0\n",
      "block4_conv2: pruned:4, protected:0\n",
      "block4_conv3: pruned:15, protected:0\n",
      "block4_conv1: pruned:2, protected:0\n",
      "block5_conv3: pruned:67, protected:0\n",
      "block5_conv2: pruned:46, protected:0\n",
      "block5_conv1: pruned:25, protected:0\n",
      "block1_conv2: pruned:4, protected:0\n",
      "block1_conv1: pruned:5, protected:0\n",
      "pruning: 4274 -> 4060, 214 get pruned\n",
      "Moved Model ACC: 0.838800\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 172s - loss: 0.0060 - acc: 0.9981 - val_loss: 0.7984 - val_acc: 0.8676\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 163s - loss: 0.0028 - acc: 0.9992 - val_loss: 0.7586 - val_acc: 0.8700\n",
      "finetuned_acc: 0.870000\n",
      "round:3 nb_neurons:4274 acc:0.838800 fintune_acc:0.870000\n",
      "\n",
      "------------------------------\n",
      "fc1: pruned:37, protected:0\n",
      "block2_conv1: pruned:1, protected:0\n",
      "block4_conv2: pruned:2, protected:0\n",
      "block4_conv3: pruned:10, protected:0\n",
      "block4_conv1: pruned:4, protected:0\n",
      "block5_conv3: pruned:67, protected:0\n",
      "block5_conv2: pruned:39, protected:0\n",
      "block5_conv1: pruned:34, protected:0\n",
      "block1_conv2: pruned:6, protected:0\n",
      "block1_conv1: pruned:3, protected:0\n",
      "pruning: 4060 -> 3857, 203 get pruned\n",
      "Moved Model ACC: 0.858800\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 169s - loss: 0.0062 - acc: 0.9984 - val_loss: 0.7729 - val_acc: 0.8639\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 162s - loss: 0.0484 - acc: 0.9864 - val_loss: 0.7069 - val_acc: 0.8553\n",
      "finetuned_acc: 0.855300\n",
      "round:4 nb_neurons:4060 acc:0.858800 fintune_acc:0.855300\n",
      "\n",
      "------------------------------\n",
      "fc1: pruned:27, protected:0\n",
      "block2_conv1: pruned:5, protected:0\n",
      "block4_conv2: pruned:1, protected:0\n",
      "block4_conv3: pruned:6, protected:0\n",
      "block4_conv1: pruned:7, protected:0\n",
      "block5_conv3: pruned:54, protected:0\n",
      "block5_conv2: pruned:37, protected:0\n",
      "block5_conv1: pruned:32, protected:0\n",
      "block1_conv2: pruned:7, protected:0\n",
      "block1_conv1: pruned:16, protected:0\n",
      "block3_conv1: pruned:1, protected:0\n",
      "pruning: 3857 -> 3664, 193 get pruned\n",
      "Moved Model ACC: 0.774200\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 159s - loss: 0.0214 - acc: 0.9939 - val_loss: 0.6855 - val_acc: 0.8655\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 152s - loss: 0.0031 - acc: 0.9992 - val_loss: 0.6824 - val_acc: 0.8680\n",
      "finetuned_acc: 0.868000\n",
      "round:5 nb_neurons:3857 acc:0.774200 fintune_acc:0.868000\n",
      "\n",
      "------------------------------\n",
      "fc1: pruned:16, protected:0\n",
      "block2_conv1: pruned:17, protected:0\n",
      "block2_conv2: pruned:1, protected:0\n",
      "block4_conv3: pruned:9, protected:0\n",
      "block4_conv1: pruned:9, protected:0\n",
      "block5_conv3: pruned:44, protected:0\n",
      "block5_conv2: pruned:37, protected:0\n",
      "block5_conv1: pruned:29, protected:0\n",
      "block1_conv2: pruned:9, protected:0\n",
      "block1_conv1: pruned:12, protected:0\n",
      "pruning: 3664 -> 3481, 183 get pruned\n",
      "Moved Model ACC: 0.668400\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 162s - loss: 0.0349 - acc: 0.9929 - val_loss: 0.7137 - val_acc: 0.8562\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 150s - loss: 0.0066 - acc: 0.9986 - val_loss: 0.7155 - val_acc: 0.8634\n",
      "finetuned_acc: 0.863400\n",
      "round:6 nb_neurons:3664 acc:0.668400 fintune_acc:0.863400\n",
      "\n",
      "------------------------------\n",
      "fc1: pruned:8, protected:0\n",
      "block2_conv1: pruned:11, protected:0\n",
      "block4_conv2: pruned:3, protected:0\n",
      "block4_conv3: pruned:9, protected:0\n",
      "block4_conv1: pruned:11, protected:0\n",
      "block5_conv3: pruned:42, protected:0\n",
      "block5_conv2: pruned:34, protected:0\n",
      "block5_conv1: pruned:37, protected:0\n",
      "block1_conv2: pruned:11, protected:0\n",
      "block1_conv1: pruned:8, protected:0\n",
      "pruning: 3481 -> 3307, 174 get pruned\n",
      "Moved Model ACC: 0.354200\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 150s - loss: 0.1937 - acc: 0.9582 - val_loss: 0.7053 - val_acc: 0.8492\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 140s - loss: 0.0163 - acc: 0.9969 - val_loss: 0.8259 - val_acc: 0.8541\n",
      "finetuned_acc: 0.854100\n",
      "round:7 nb_neurons:3481 acc:0.354200 fintune_acc:0.854100\n",
      "\n",
      "------------------------------\n",
      "fc1: pruned:3, protected:0\n",
      "block2_conv1: pruned:15, protected:0\n",
      "block2_conv2: pruned:5, protected:0\n",
      "block4_conv2: pruned:5, protected:0\n",
      "block4_conv3: pruned:6, protected:0\n",
      "block4_conv1: pruned:6, protected:0\n",
      "block5_conv3: pruned:34, protected:0\n",
      "block5_conv2: pruned:29, protected:0\n",
      "block5_conv1: pruned:36, protected:0\n",
      "block1_conv2: pruned:13, protected:0\n",
      "block1_conv1: pruned:7, protected:0\n",
      "block3_conv1: pruned:4, protected:0\n",
      "block3_conv2: pruned:2, protected:0\n",
      "pruning: 3307 -> 3142, 165 get pruned\n",
      "Moved Model ACC: 0.139000\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 142s - loss: 0.6815 - acc: 0.8253 - val_loss: 0.7625 - val_acc: 0.8058\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 131s - loss: 0.1457 - acc: 0.9637 - val_loss: 0.8204 - val_acc: 0.8202\n",
      "finetuned_acc: 0.820200\n",
      "round:8 nb_neurons:3307 acc:0.139000 fintune_acc:0.820200\n",
      "\n",
      "------------------------------\n",
      "fc1: pruned:2, protected:0\n",
      "block2_conv1: pruned:22, protected:0\n",
      "block2_conv2: pruned:13, protected:0\n",
      "block4_conv2: pruned:7, protected:0\n",
      "block4_conv3: pruned:5, protected:0\n",
      "block4_conv1: pruned:5, protected:0\n",
      "block5_conv3: pruned:28, protected:0\n",
      "block5_conv2: pruned:36, protected:0\n",
      "block5_conv1: pruned:27, protected:0\n",
      "block1_conv2: pruned:6, protected:0\n",
      "block1_conv1: pruned:2, protected:0\n",
      "block3_conv1: pruned:3, protected:0\n",
      "block3_conv3: pruned:1, protected:0\n",
      "pruning: 3142 -> 2985, 157 get pruned\n"
     ]
    }
   ],
   "source": [
    "\n",
    "method = \"AAWS\"\n",
    "mode = \"global\"\n",
    "#pruning_ratio = 0.03\n",
    "for pruning_ratio in [0.05]:\n",
    "    prune_round = 1\n",
    "    target_layers = [\"block1_conv1\",\"block1_conv2\",\n",
    "                   \"block2_conv1\",\"block2_conv2\",\n",
    "                   \"block3_conv1\",\"block3_conv2\",\"block3_conv3\",\n",
    "                   \"block4_conv1\",\"block4_conv2\",\"block4_conv3\",\n",
    "                   \"block5_conv1\",\"block5_conv2\",\"block5_conv3\",\"fc1\"]\n",
    "\n",
    "    while True:\n",
    "        model_name = \"vgg_cifar10_\"+str(prune_round-1)+\".h5\"\n",
    "        model = load_model(model_name)\n",
    "        \n",
    "        protect_dict = {\"block1_conv1\":32,\n",
    "                        \"block1_conv2\":32,\n",
    "                        \"block2_conv1\":64,\n",
    "                        \"block2_conv2\":64,\n",
    "                        \"block3_conv1\":128,\n",
    "                        \"block3_conv2\":128,\n",
    "                        \"block3_conv3\":128,\n",
    "                        \"block4_conv1\":256,\n",
    "                        \"block4_conv2\":256,\n",
    "                        \"block4_conv3\":256,\n",
    "                        \"block5_conv1\":256,\n",
    "                        \"block5_conv2\":256,\n",
    "                        \"block5_conv3\":256,\n",
    "                        \"fc1\":100}\n",
    "        netscore = NetScore(mode=mode)\n",
    "\n",
    "        if method == 'AAWS':\n",
    "            for target_layer in target_layers:\n",
    "                if target_layer[:2]=='fc':\n",
    "                    w,b = model.get_layer(\"predications\").get_weights()\n",
    "                    weight_sum = np.mean(np.abs(w), axis=1)\n",
    "                    weight_sum = weight_sum/np.mean(weight_sum)\n",
    "                else:\n",
    "                    w,b = model.get_layer(target_layer).get_weights()\n",
    "                    weight_sum = np.mean(np.abs(w), axis=(0,1,2))\n",
    "                    weight_sum = weight_sum/np.mean(weight_sum)\n",
    "\n",
    "                for neuron_id in range(weight_sum.shape[0]):\n",
    "                    netscore.add(target_layer, neuron_id, weight_sum[neuron_id])\n",
    "        if mode=='layer_wise':\n",
    "            neuron_num = netscore.neuron_num\n",
    "            p = get_num_by_ratio(neuron_num, pruning_ratio)\n",
    "\n",
    "        else:\n",
    "            p = pruning_ratio\n",
    "            \n",
    "        netscore.sort()\n",
    "        redundant_neurons = netscore.get_redundant_neurons(p)\n",
    "\n",
    "\n",
    "        for neuron in redundant_neurons:\n",
    "            neuron_layer, neuron_id, neuron_score = neuron\n",
    "            if neuron_layer[:2]=='fc':\n",
    "                w,b = model.get_layer(\"predications\").get_weights()\n",
    "                w[neuron_id,:] = 0.\n",
    "                model.get_layer(\"predications\").set_weights([w,b])\n",
    "\n",
    "                w,b = model.get_layer(neuron_layer).get_weights()\n",
    "                w[:, neuron_id] = 0.\n",
    "                b[neuron_id] = 0.\n",
    "                model.get_layer(neuron_layer).set_weights([w,b])\n",
    "\n",
    "            else:\n",
    "            # do pruning to target_layer\n",
    "                w,b = model.get_layer(neuron_layer).get_weights()\n",
    "                w[:,:,:,neuron_id] = 0.\n",
    "                b[neuron_id] = 0.\n",
    "                model.get_layer(neuron_layer).set_weights([w,b])\n",
    "            # assign bn\n",
    "            bn_gamma, bn_beta, bn_mean, bn_std = model.get_layer(next_layer(neuron_layer)).get_weights()\n",
    "            bn_gamma[neuron_id] = 0.\n",
    "            bn_beta[neuron_id] = 0.\n",
    "            bn_mean[neuron_id] = 0.\n",
    "            bn_std[neuron_id] = 1.\n",
    "            model.get_layer(next_layer(neuron_layer)).set_weights([bn_gamma, bn_beta, bn_mean, bn_std])\n",
    "\n",
    "        netscore.print_info()\n",
    "        model = MoveNetwork(model, \"block5_conv3\")\n",
    "\n",
    "\n",
    "\n",
    "    # fine-tune\n",
    "\n",
    "        model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "        print(\"Moved Model ACC: %f\"%score[1])\n",
    "        model.fit(X_train, Y_train,batch_size=50,nb_epoch=1,validation_data=(X_test, Y_test),shuffle=True)\n",
    "\n",
    "        sgd = SGD(lr=0.01, momentum=0.9, decay=5*1e-4,\n",
    "                     nesterov=True)\n",
    "        model.compile(loss = 'categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "        model.fit(X_train, Y_train,batch_size=50,nb_epoch=1,validation_data=(X_test, Y_test),shuffle=True)\n",
    "\n",
    "        model.save(\"vgg_cifar10_\"+str(prune_round)+\".h5\")\n",
    "\n",
    "        fine_score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "        print(\"finetuned_acc: %f\"%fine_score[1])\n",
    "        record = \"round:%d nb_neurons:%d acc:%f fintune_acc:%f\\n\"%(prune_round, netscore.total_num, score[1],fine_score[1])\n",
    "        print(record)\n",
    "        print(\"------------------------------\")\n",
    "        prune_round+=1\n",
    "        if fine_score[1]<0.5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pruning: 4736 -> 4736, 0 get pruned\n",
      "finetuned_acc: 0.873200\n",
      "ratio:0.000000 nb_neurons:4736 fintune_acc:0.873200\n",
      "\n",
      "------------------------------\n",
      "fc1: pruned:10, protected:0\n",
      "block2_conv1: pruned:3, protected:0\n",
      "block2_conv2: pruned:3, protected:0\n",
      "block4_conv2: pruned:10, protected:0\n",
      "block4_conv3: pruned:10, protected:0\n",
      "block4_conv1: pruned:10, protected:0\n",
      "block5_conv3: pruned:10, protected:0\n",
      "block5_conv2: pruned:10, protected:0\n",
      "block5_conv1: pruned:10, protected:0\n",
      "block1_conv2: pruned:1, protected:0\n",
      "block1_conv1: pruned:1, protected:0\n",
      "block3_conv1: pruned:5, protected:0\n",
      "block3_conv3: pruned:5, protected:0\n",
      "block3_conv2: pruned:5, protected:0\n",
      "pruning: 4736 -> 4643, 93 get pruned\n",
      "finetuned_acc: 0.862500\n",
      "ratio:0.020000 nb_neurons:4736 fintune_acc:0.862500\n",
      "\n",
      "------------------------------\n",
      "fc1: pruned:20, protected:0\n",
      "block2_conv1: pruned:5, protected:0\n",
      "block2_conv2: pruned:5, protected:0\n",
      "block4_conv2: pruned:20, protected:0\n",
      "block4_conv3: pruned:20, protected:0\n",
      "block4_conv1: pruned:20, protected:0\n",
      "block5_conv3: pruned:20, protected:0\n",
      "block5_conv2: pruned:20, protected:0\n",
      "block5_conv1: pruned:20, protected:0\n",
      "block1_conv2: pruned:3, protected:0\n",
      "block1_conv1: pruned:3, protected:0\n",
      "block3_conv1: pruned:10, protected:0\n",
      "block3_conv3: pruned:10, protected:0\n",
      "block3_conv2: pruned:10, protected:0\n",
      "pruning: 4736 -> 4550, 186 get pruned\n",
      "finetuned_acc: 0.846300\n",
      "ratio:0.040000 nb_neurons:4736 fintune_acc:0.846300\n",
      "\n",
      "------------------------------\n",
      "fc1: pruned:31, protected:0\n",
      "block2_conv1: pruned:8, protected:0\n",
      "block2_conv2: pruned:8, protected:0\n",
      "block4_conv2: pruned:31, protected:0\n",
      "block4_conv3: pruned:31, protected:0\n",
      "block4_conv1: pruned:31, protected:0\n",
      "block5_conv3: pruned:31, protected:0\n",
      "block5_conv2: pruned:31, protected:0\n",
      "block5_conv1: pruned:31, protected:0\n",
      "block1_conv2: pruned:4, protected:0\n",
      "block1_conv1: pruned:4, protected:0\n",
      "block3_conv1: pruned:15, protected:0\n",
      "block3_conv3: pruned:15, protected:0\n",
      "block3_conv2: pruned:15, protected:0\n",
      "pruning: 4736 -> 4450, 286 get pruned\n",
      "finetuned_acc: 0.836500\n",
      "ratio:0.060000 nb_neurons:4736 fintune_acc:0.836500\n",
      "\n",
      "------------------------------\n",
      "fc1: pruned:41, protected:0\n",
      "block2_conv1: pruned:10, protected:0\n",
      "block2_conv2: pruned:10, protected:0\n",
      "block4_conv2: pruned:41, protected:0\n",
      "block4_conv3: pruned:41, protected:0\n",
      "block4_conv1: pruned:41, protected:0\n",
      "block5_conv3: pruned:41, protected:0\n",
      "block5_conv2: pruned:41, protected:0\n",
      "block5_conv1: pruned:41, protected:0\n",
      "block1_conv2: pruned:5, protected:0\n",
      "block1_conv1: pruned:5, protected:0\n",
      "block3_conv1: pruned:20, protected:0\n",
      "block3_conv3: pruned:20, protected:0\n",
      "block3_conv2: pruned:20, protected:0\n",
      "pruning: 4736 -> 4359, 377 get pruned\n",
      "finetuned_acc: 0.810800\n",
      "ratio:0.080000 nb_neurons:4736 fintune_acc:0.810800\n",
      "\n",
      "------------------------------\n",
      "fc1: pruned:51, protected:0\n",
      "block2_conv1: pruned:13, protected:0\n",
      "block2_conv2: pruned:13, protected:0\n",
      "block4_conv2: pruned:51, protected:0\n",
      "block4_conv3: pruned:51, protected:0\n",
      "block4_conv1: pruned:51, protected:0\n",
      "block5_conv3: pruned:51, protected:0\n",
      "block5_conv2: pruned:51, protected:0\n",
      "block5_conv1: pruned:51, protected:0\n",
      "block1_conv2: pruned:6, protected:0\n",
      "block1_conv1: pruned:6, protected:0\n",
      "block3_conv1: pruned:26, protected:0\n",
      "block3_conv3: pruned:26, protected:0\n",
      "block3_conv2: pruned:26, protected:0\n",
      "pruning: 4736 -> 4263, 473 get pruned\n",
      "finetuned_acc: 0.798700\n",
      "ratio:0.100000 nb_neurons:4736 fintune_acc:0.798700\n",
      "\n",
      "------------------------------\n",
      "fc1: pruned:61, protected:0\n",
      "block2_conv1: pruned:15, protected:0\n",
      "block2_conv2: pruned:15, protected:0\n",
      "block4_conv2: pruned:61, protected:0\n",
      "block4_conv3: pruned:61, protected:0\n",
      "block4_conv1: pruned:61, protected:0\n",
      "block5_conv3: pruned:61, protected:0\n",
      "block5_conv2: pruned:61, protected:0\n",
      "block5_conv1: pruned:61, protected:0\n",
      "block1_conv2: pruned:8, protected:0\n",
      "block1_conv1: pruned:8, protected:0\n",
      "block3_conv1: pruned:31, protected:0\n",
      "block3_conv3: pruned:31, protected:0\n",
      "block3_conv2: pruned:31, protected:0\n",
      "pruning: 4736 -> 4170, 566 get pruned\n",
      "finetuned_acc: 0.791000\n",
      "ratio:0.120000 nb_neurons:4736 fintune_acc:0.791000\n",
      "\n",
      "------------------------------\n",
      "fc1: pruned:72, protected:0\n",
      "block2_conv1: pruned:18, protected:0\n",
      "block2_conv2: pruned:18, protected:0\n",
      "block4_conv2: pruned:72, protected:0\n",
      "block4_conv3: pruned:72, protected:0\n",
      "block4_conv1: pruned:72, protected:0\n",
      "block5_conv3: pruned:72, protected:0\n",
      "block5_conv2: pruned:72, protected:0\n",
      "block5_conv1: pruned:72, protected:0\n",
      "block1_conv2: pruned:9, protected:0\n",
      "block1_conv1: pruned:9, protected:0\n",
      "block3_conv1: pruned:36, protected:0\n",
      "block3_conv3: pruned:36, protected:0\n",
      "block3_conv2: pruned:36, protected:0\n",
      "pruning: 4736 -> 4070, 666 get pruned\n",
      "finetuned_acc: 0.785100\n",
      "ratio:0.140000 nb_neurons:4736 fintune_acc:0.785100\n",
      "\n",
      "------------------------------\n",
      "fc1: pruned:82, protected:0\n",
      "block2_conv1: pruned:20, protected:0\n",
      "block2_conv2: pruned:20, protected:0\n",
      "block4_conv2: pruned:82, protected:0\n",
      "block4_conv3: pruned:82, protected:0\n",
      "block4_conv1: pruned:82, protected:0\n",
      "block5_conv3: pruned:82, protected:0\n",
      "block5_conv2: pruned:82, protected:0\n",
      "block5_conv1: pruned:82, protected:0\n",
      "block1_conv2: pruned:10, protected:0\n",
      "block1_conv1: pruned:10, protected:0\n",
      "block3_conv1: pruned:41, protected:0\n",
      "block3_conv3: pruned:41, protected:0\n",
      "block3_conv2: pruned:41, protected:0\n",
      "pruning: 4736 -> 3979, 757 get pruned\n",
      "finetuned_acc: 0.745100\n",
      "ratio:0.160000 nb_neurons:4736 fintune_acc:0.745100\n",
      "\n",
      "------------------------------\n",
      "fc1: pruned:92, protected:0\n",
      "block2_conv1: pruned:23, protected:0\n",
      "block2_conv2: pruned:23, protected:0\n",
      "block4_conv2: pruned:92, protected:0\n",
      "block4_conv3: pruned:92, protected:0\n",
      "block4_conv1: pruned:92, protected:0\n",
      "block5_conv3: pruned:92, protected:0\n",
      "block5_conv2: pruned:92, protected:0\n",
      "block5_conv1: pruned:92, protected:0\n",
      "block1_conv2: pruned:12, protected:0\n",
      "block1_conv1: pruned:12, protected:0\n",
      "block3_conv1: pruned:46, protected:0\n",
      "block3_conv3: pruned:46, protected:0\n",
      "block3_conv2: pruned:46, protected:0\n",
      "pruning: 4736 -> 3884, 852 get pruned\n",
      "finetuned_acc: 0.710100\n",
      "ratio:0.180000 nb_neurons:4736 fintune_acc:0.710100\n",
      "\n",
      "------------------------------\n",
      "fc1: pruned:102, protected:0\n",
      "block2_conv1: pruned:26, protected:0\n",
      "block2_conv2: pruned:26, protected:0\n",
      "block4_conv2: pruned:102, protected:0\n",
      "block4_conv3: pruned:102, protected:0\n",
      "block4_conv1: pruned:102, protected:0\n",
      "block5_conv3: pruned:102, protected:0\n",
      "block5_conv2: pruned:102, protected:0\n",
      "block5_conv1: pruned:102, protected:0\n",
      "block1_conv2: pruned:13, protected:0\n",
      "block1_conv1: pruned:13, protected:0\n",
      "block3_conv1: pruned:51, protected:0\n",
      "block3_conv3: pruned:51, protected:0\n",
      "block3_conv2: pruned:51, protected:0\n",
      "pruning: 4736 -> 3791, 945 get pruned\n",
      "finetuned_acc: 0.697600\n",
      "ratio:0.200000 nb_neurons:4736 fintune_acc:0.697600\n",
      "\n",
      "------------------------------\n",
      "fc1: pruned:113, protected:0\n",
      "block2_conv1: pruned:28, protected:0\n",
      "block2_conv2: pruned:28, protected:0\n",
      "block4_conv2: pruned:113, protected:0\n",
      "block4_conv3: pruned:113, protected:0\n",
      "block4_conv1: pruned:113, protected:0\n",
      "block5_conv3: pruned:113, protected:0\n",
      "block5_conv2: pruned:113, protected:0\n",
      "block5_conv1: pruned:113, protected:0\n",
      "block1_conv2: pruned:14, protected:0\n",
      "block1_conv1: pruned:14, protected:0\n",
      "block3_conv1: pruned:56, protected:0\n",
      "block3_conv3: pruned:56, protected:0\n",
      "block3_conv2: pruned:56, protected:0\n",
      "pruning: 4736 -> 3693, 1043 get pruned\n"
     ]
    }
   ],
   "source": [
    "## no fine-tune\n",
    "\n",
    "mode = \"layer_wise\"\n",
    "for method in [\"AAWS\"]:\n",
    "    for pruning_ratio in [0.,0.02,0.04,0.06,0.08,0.10,0.12,0.14,0.16,0.18,0.20,0.22,0.24,0.26,0.28,0.30]:\n",
    "        prune_round = 1\n",
    "        target_layers = [\"block1_conv1\",\"block1_conv2\",\n",
    "                   \"block2_conv1\",\"block2_conv2\",\n",
    "                   \"block3_conv1\",\"block3_conv2\",\"block3_conv3\",\n",
    "                   \"block4_conv1\",\"block4_conv2\",\"block4_conv3\",\n",
    "                   \"block5_conv1\",\"block5_conv2\",\"block5_conv3\",\"fc1\"]\n",
    "\n",
    "\n",
    "        model_name = \"vgg_cifar10_\"+str(prune_round-1)+\".h5\"\n",
    "        model = load_model(model_name)\n",
    "        netscore = NetScore(mode=mode)\n",
    "\n",
    "        if method == 'AAWS':\n",
    "            for target_layer in target_layers:\n",
    "                if target_layer[:2]=='fc':\n",
    "                    w,b = model.get_layer(\"predications\").get_weights()\n",
    "                    weight_sum = np.mean(np.abs(w), axis=1)\n",
    "                    weight_sum = weight_sum/np.mean(weight_sum)\n",
    "                else:\n",
    "                    w,b = model.get_layer(target_layer).get_weights()\n",
    "                    weight_sum = np.mean(np.abs(w), axis=(0,1,2))\n",
    "                    weight_sum = weight_sum/np.mean(weight_sum)\n",
    "\n",
    "                for neuron_id in range(weight_sum.shape[0]):\n",
    "                    netscore.add(target_layer, neuron_id, weight_sum[neuron_id])\n",
    "        elif method == \"mean\" or method == \"sigma\":\n",
    "            mean_dict = {}\n",
    "            std_dict = {}\n",
    "            all_data = {}\n",
    "            for target_layer in target_layers:\n",
    "\n",
    "                f = h5py.File(\"prune\"+str(prune_round)+'_'+target_layer+'_cifar10.h5', 'r')\n",
    "                raw_stat = {}\n",
    "\n",
    "                for k in f.keys():\n",
    "                    raw_stat[k] = f[k][:]\n",
    "\n",
    "\n",
    "                all_data[target_layer] = raw_stat\n",
    "                # put activation together\n",
    "                act_data = []\n",
    "                for k in raw_stat.keys():\n",
    "                    act_data += list(raw_stat[k])\n",
    "\n",
    "                act_data = np.array(act_data)\n",
    "\n",
    "                mean_dict[target_layer] = np.mean(act_data, axis=0)\n",
    "                std_dict[target_layer] = np.std(act_data, axis=0)\n",
    "                mean_dict[target_layer] = mean_dict[target_layer]/np.mean(mean_dict[target_layer])\n",
    "                std_dict[target_layer] = std_dict[target_layer]/np.mean(std_dict[target_layer])\n",
    "        if method == 'sigma':\n",
    "            for target_layer in target_layers:\n",
    "                for neuron_id in range(std_dict[target_layer].shape[0]):\n",
    "                    netscore.add(target_layer, neuron_id, std_dict[target_layer][neuron_id])\n",
    "\n",
    "        elif method == 'mean':\n",
    "            for target_layer in target_layers:\n",
    "                for neuron_id in range(mean_dict[target_layer].shape[0]):\n",
    "                    netscore.add(target_layer, neuron_id, mean_dict[target_layer][neuron_id])\n",
    "                    \n",
    "        if mode=='layer_wise':\n",
    "            neuron_num = netscore.neuron_num\n",
    "            p = get_num_by_ratio(neuron_num, pruning_ratio)\n",
    "\n",
    "        else:\n",
    "            p = pruning_ratio\n",
    "            \n",
    "        netscore.sort()\n",
    "        redundant_neurons = netscore.get_redundant_neurons(p)\n",
    "\n",
    "\n",
    "        for neuron in redundant_neurons:\n",
    "            neuron_layer, neuron_id, neuron_score = neuron\n",
    "            if neuron_layer[:2]=='fc':\n",
    "                w,b = model.get_layer(\"predications\").get_weights()\n",
    "                w[neuron_id,:] = 0.\n",
    "                model.get_layer(\"predications\").set_weights([w,b])\n",
    "\n",
    "                w,b = model.get_layer(neuron_layer).get_weights()\n",
    "                w[:, neuron_id] = 0.\n",
    "                b[neuron_id] = 0.\n",
    "                model.get_layer(neuron_layer).set_weights([w,b])\n",
    "\n",
    "            else:\n",
    "            # do pruning to target_layer\n",
    "                w,b = model.get_layer(neuron_layer).get_weights()\n",
    "                w[:,:,:,neuron_id] = 0.\n",
    "                b[neuron_id] = 0.\n",
    "                model.get_layer(neuron_layer).set_weights([w,b])\n",
    "            # assign bn\n",
    "            bn_gamma, bn_beta, bn_mean, bn_std = model.get_layer(next_layer(neuron_layer)).get_weights()\n",
    "            bn_gamma[neuron_id] = 0.\n",
    "            bn_beta[neuron_id] = 0.\n",
    "            bn_mean[neuron_id] = 0.\n",
    "            bn_std[neuron_id] = 1.\n",
    "            model.get_layer(next_layer(neuron_layer)).set_weights([bn_gamma, bn_beta, bn_mean, bn_std])\n",
    "\n",
    "        netscore.print_info()\n",
    "        \n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "        fine_score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "        print(\"finetuned_acc: %f\"%fine_score[1])\n",
    "        record = \"ratio:%f nb_neurons:%d fintune_acc:%f\\n\"%(pruning_ratio, netscore.total_num,fine_score[1])\n",
    "        print(record)\n",
    "        print(\"------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 14s    \n",
      "14.1828458309\n",
      "10000/10000 [==============================] - 7s     \n",
      "7.8745071888\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from keras.models import load_model\n",
    "model_name = \"vgg_cifar10_\"+str(0)+\".h5\"\n",
    "model = load_model(model_name)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "t1 = time.time()\n",
    "fine_score = model.evaluate(X_test, Y_test, verbose=1)\n",
    "t2 = time.time()\n",
    "print(t2-t1)\n",
    "\n",
    "model_name = \"vgg_cifar10_\"+str(7)+\".h5\"\n",
    "model = load_model(model_name)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "t1 = time.time()\n",
    "fine_score = model.evaluate(X_test, Y_test, verbose=1)\n",
    "t2 = time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
