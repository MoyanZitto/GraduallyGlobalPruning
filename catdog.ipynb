{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense, Convolution2D, BatchNormalization, MaxPooling2D, Input, Flatten\n",
    "from keras.models import Sequential\n",
    "import copy\n",
    "import keras.backend as K\n",
    "import h5py\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# load model and data\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import Dense\n",
    "from keras.models import Model\n",
    "\n",
    "def get_model():\n",
    "    model = VGG16(include_top=True, weights='imagenet')\n",
    "    out = Dense(2, activation='softmax', name='decision')(model.get_layer('fc2').output)\n",
    "\n",
    "    model = Model(model.input, out)\n",
    "\n",
    "\n",
    "    for layer in model.layers:\n",
    "        print(layer.name)\n",
    "        layer.trainable=False\n",
    "\n",
    "    model.layers[-1].trainable=True\n",
    "    return model\n",
    "\n",
    "def data_gen(path, batch_size=20):\n",
    "    batch = []\n",
    "    labels = []\n",
    "    while 1:\n",
    "        f = open(path)\n",
    "        for line in f:\n",
    "            img_path, label = line.strip().split(' ')\n",
    "            img = load_img(img_path, target_size=(224, 224))\n",
    "            img = img_to_array(img)\n",
    "            batch.append(img)\n",
    "            labels.append(int(label))\n",
    "            if len(batch)==batch_size:\n",
    "                batch = preprocess_input(np.array(batch))\n",
    "                labels = to_categorical(np.array(labels))\n",
    "                yield((batch, labels))\n",
    "                batch = []\n",
    "                labels =[]\n",
    "        f.close()\n",
    "        \n",
    "def categorical_to_scalar(cat):\n",
    "    labels = np.zeros(cat.shape[0])\n",
    "    for i in range(cat.shape[0]):\n",
    "        labels[i] = np.argmax(cat[i])\n",
    "    return labels\n",
    "\n",
    "def MoveNetwork(net, layer_before_flatten=None):\n",
    "    \"\"\"\n",
    "    This function will extract a subnetwork from the original network, only work with plain networks.\n",
    "    A plain network is a network with only one input and one output, contains only:\n",
    "        - Convolution2D\n",
    "        - MaxPooling2D/AveragePooling2D\n",
    "        - BatchNormalization\n",
    "        - Dropout\n",
    "        - Flatten\n",
    "        - Dense\n",
    "        - Activation\n",
    "    Neurons whose parameters contain at least a non-zero value would be extracted.\n",
    "\n",
    "    params:\n",
    "        - net: A Keras model\n",
    "        - layer_before_flatten: str, the name of the convolution layer just before the flatten layer.\n",
    "    returns:\n",
    "        - A sub-network extracted from the original one.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    weight_list = []\n",
    "    for layer in net.layers:\n",
    "        if layer.__class__.__name__ == \"InputLayer\":\n",
    "            continue\n",
    "        if layer.__class__.__name__ == 'Convolution2D':\n",
    "            if len(model.layers)==0:\n",
    "                w,b = layer.get_weights()\n",
    "                save_id = []\n",
    "                for fid in range(w.shape[3]):\n",
    "                    if np.sum(np.abs(w[:,:,:,fid]))!=0.:\n",
    "                        save_id.append(fid)\n",
    "                w_new = np.zeros(w.shape[:3]+(len(save_id),))\n",
    "                b_new = np.zeros(len(save_id))\n",
    "\n",
    "                i = 0\n",
    "                for fid in save_id:\n",
    "                    w_new[:,:,:,i] = w[:,:,:,fid]\n",
    "                    b_new[i] = b[fid]\n",
    "                    i+=1\n",
    "                config = layer.get_config()\n",
    "                new_layer = Convolution2D(len(save_id), config['nb_col'], config['nb_row'],\n",
    "                                          activation=config['activation'], border_mode=config['border_mode'],\n",
    "                                          name=config['name'], input_shape=layer.input_shape[1:])\n",
    "                weight_list.append([w_new, b_new])\n",
    "                model.add(new_layer)\n",
    "                prev_save_id = copy.deepcopy(save_id)\n",
    "            else:\n",
    "                nb_channels = len(prev_save_id)\n",
    "                w,b = layer.get_weights()\n",
    "                save_id = []\n",
    "                for fid in range(w.shape[3]):\n",
    "                    if np.sum(np.abs(w[:,:,:,fid]))>0.0000001:\n",
    "                        save_id.append(fid)\n",
    "\n",
    "                w_new = np.zeros(w.shape[:2]+(nb_channels,len(save_id)))\n",
    "                b_new = np.zeros(len(save_id))\n",
    "\n",
    "                i = 0\n",
    "                for fid in save_id:\n",
    "                    w_new[:,:,:,i] = w[:,:,prev_save_id,fid]\n",
    "                    b_new[i] = b[fid]\n",
    "                    i+=1\n",
    "                config = layer.get_config()\n",
    "                new_layer = Convolution2D(len(save_id), config['nb_col'], config['nb_row'],\n",
    "                          activation=config['activation'], border_mode=config['border_mode'],\n",
    "                          name=config['name'])\n",
    "                weight_list.append([w_new, b_new])\n",
    "                model.add(new_layer)\n",
    "                prev_save_id = copy.deepcopy(save_id)\n",
    "\n",
    "        elif layer.__class__.__name__ == 'BatchNormalization':\n",
    "            gamma, beta, mean, std = layer.get_weights()\n",
    "            nb_channels = len(prev_save_id)\n",
    "            gamma_new = np.zeros(len(prev_save_id))\n",
    "            beta_new = np.zeros(len(prev_save_id))\n",
    "            mean_new = np.zeros(len(prev_save_id))\n",
    "            std_new = np.zeros(len(prev_save_id))\n",
    "\n",
    "\n",
    "            gamma_new = gamma[prev_save_id]\n",
    "            beta_new = beta[prev_save_id]\n",
    "            mean_new = mean[prev_save_id]\n",
    "            std_new = std[prev_save_id]\n",
    "\n",
    "\n",
    "            config = layer.get_config()\n",
    "            new_layer = BatchNormalization(name=config['name'], axis=config['axis'])\n",
    "            weight_list.append([gamma_new, beta_new, mean_new, std_new])\n",
    "            model.add(new_layer)\n",
    "            \n",
    "        elif layer.__class__.__name__ == 'MaxPooling2D' or layer.__class__.__name__ == 'AveragePooling2D':\n",
    "            config = layer.get_config()\n",
    "            new_layer = MaxPooling2D(pool_size = config['pool_size'], strides = config['pool_size'], name = config['name'])\n",
    "            model.add(new_layer)\n",
    "            weight_list.append(None)\n",
    "\n",
    "        elif layer.__class__.__name__=='Flatten':\n",
    "            new_layer = Flatten(name = 'flatten')\n",
    "            model.add(new_layer)\n",
    "            weight_list.append(None)\n",
    "\n",
    "        elif layer.__class__.__name__=='Dense':\n",
    "            if model.layers[-1].__class__.__name__=='Flatten':\n",
    "                last_layer = net.get_layer(layer_before_flatten)\n",
    "                last_layer_output_shape = K.int_shape(last_layer.output)[1:]\n",
    "                fake_input = np.zeros(last_layer_output_shape)\n",
    "                fake_input[:,:,prev_save_id] = 1.\n",
    "                fake_input = np.expand_dims(fake_input, axis=0)\n",
    "\n",
    "                input_tensor = K.placeholder(fake_input.shape)\n",
    "                output_tensor = Flatten()(input_tensor)\n",
    "                get_indicator = K.function([input_tensor, K.learning_phase()],\n",
    "                          [output_tensor])\n",
    "                indicators = get_indicator([fake_input, 0])[0]\n",
    "                indicators = np.squeeze(indicators)\n",
    "                indicator_index = []\n",
    "                for i in range(indicators.shape[0]):\n",
    "                    if indicators[i]==1.:\n",
    "                        indicator_index.append(i)\n",
    "                save_id = []\n",
    "                w,b = layer.get_weights()\n",
    "                \n",
    "                for i in range(w.shape[1]):\n",
    "                    if np.sum(np.abs(w[:,i]))!=0.:\n",
    "                        save_id.append(i)\n",
    "                w_new = w[indicator_index,:]\n",
    "                w_new = w_new[:,save_id]\n",
    "                b_new = b[save_id]\n",
    "                config = layer.get_config()\n",
    "                new_layer = Dense(len(save_id), activation=config['activation'], name=config['name'])\n",
    "                weight_list.append([w_new,b_new])\n",
    "                model.add(new_layer)\n",
    "                prev_save_id = copy.deepcopy(save_id)\n",
    "                \n",
    "\n",
    "            else:\n",
    "                w,b = layer.get_weights()\n",
    "                save_id = []\n",
    "                for i in range(w.shape[1]):\n",
    "                    if np.sum(np.abs(w[:,i]))!=0.:\n",
    "                        save_id.append(i)\n",
    "                w_new = w[:,save_id]\n",
    "                b_new = b[save_id]\n",
    "                w_new = w_new[prev_save_id,:]\n",
    "\n",
    "                config = layer.get_config()\n",
    "                new_layer = Dense(len(save_id), activation=config['activation'], name=config['name'])\n",
    "                weight_list.append([w_new,b_new])\n",
    "                model.add(new_layer)\n",
    "                prev_save_id = copy.deepcopy(save_id)\n",
    "\n",
    "            \n",
    "\n",
    "        elif layer.__class__.__name__=='Dropout':\n",
    "            config = layer.get_config()\n",
    "            new_layer = Dropout(config['p'])\n",
    "            weight_list.append(None)\n",
    "            model.add(new_layer)\n",
    "\n",
    "            \n",
    "        else:\n",
    "            print(\"Exist unknown layer type:%s\"%layer.name)\n",
    "\n",
    "\n",
    "    for i in range(len(model.layers)):\n",
    "        if weight_list[i] == None:\n",
    "            continue\n",
    "        model.layers[i].set_weights(weight_list[i])\n",
    "    return model\n",
    "\n",
    "\n",
    "def next_layer(target_layer):\n",
    "    \"\"\"\n",
    "    This function is set to find the BatchNormalization after the conv layers and fc layers\n",
    "    you may always need to define this function if your network contains BatchNormalization layer\n",
    "\n",
    "    \"\"\"\n",
    "    next_layers={\"block1_conv1\":\"block1_bn1\",\n",
    "               \"block1_conv2\":\"block1_bn2\",\n",
    "               \"block2_conv1\":\"block2_bn1\",\n",
    "               \"block2_conv2\":\"block2_bn2\",\n",
    "               \"block3_conv1\":\"block3_bn1\",\n",
    "               \"block3_conv2\":\"block3_bn2\",\n",
    "               \"block3_conv3\":\"block3_bn3\",\n",
    "               \"block4_conv1\":\"block4_bn1\",\n",
    "               \"block4_conv2\":\"block4_bn2\",\n",
    "               \"block4_conv3\":\"block4_bn3\",\n",
    "               \"block5_conv1\":\"block5_bn1\",\n",
    "               \"block5_conv2\":\"block5_bn2\",\n",
    "               \"block5_conv3\":\"block5_bn3\",\n",
    "               \"fc1\":\"fc_bn1\"\n",
    "          }\n",
    "    return next_layers[target_layer]\n",
    "\n",
    "\n",
    "\n",
    "def get_activation(model, prefix, X_train, y_train):\n",
    "    \"\"\"\n",
    "    This function obtain the activation of each layer when using data-dependent neuron selection strategy and\n",
    "    save them in several hdf5 files.\n",
    "    params:\n",
    "        model: the network\n",
    "        prefix: the name\n",
    "    \"\"\"\n",
    "\n",
    "    for target_layer in model.layers:\n",
    "        if target_layer.__class__.__name__ in [\"Convolution2D\", \"Dense\"]:\n",
    "            input_tensor = model.input\n",
    "            output_tensor = target_layer.output\n",
    "\n",
    "            get_outputs = K.function([input_tensor, K.learning_phase()],\n",
    "                              [output_tensor])\n",
    "\n",
    "            raw_stat = {}\n",
    "            for s in range(X_train.shape[0]):\n",
    "                img = np.expand_dims(X_train[s], axis=0)\n",
    "                label = y_train[s]\n",
    "                output = get_outputs([img, 0])[0]\n",
    "                output = np.squeeze(output)\n",
    "                if target_layer.__class__.__name__=='Dense':\n",
    "                    output = output\n",
    "                elif target_layer.__class__.__name__=='Convolution2D':\n",
    "                    output = np.mean(output, axis = (0,1))\n",
    "                assert(len(output.shape)==1)\n",
    "\n",
    "                if raw_stat.has_key(int(label)):\n",
    "                    raw_stat[int(label)].append(output)\n",
    "                else:\n",
    "                    raw_stat[int(label)]= [output]\n",
    "\n",
    "\n",
    "            f = h5py.File(save_name,'w')\n",
    "            for k in raw_stat.keys():\n",
    "                f.create_dataset(name=str(k), data=raw_stat[k])\n",
    "            f.close()\n",
    "\n",
    "\n",
    "class NetScore(object):\n",
    "    \"\"\"\n",
    "    This class record the contribution score of all neurons in network. And provide a series of\n",
    "    methods to get the redundant neurons.\n",
    "\n",
    "    You may set some properties when instantiating.\n",
    "\n",
    "    protect_dict: dict, mapping layer_name -> nb_neurons to be protected. If given, at least 'protect_dict[layer_name]'' neurons\n",
    "                  at 'layer_name' would be preserved.\n",
    "    model: str, \"global\" or \"layer_wise\", default \"global\". If set to \"layer_wise\", the number of redundant neurons in each layer\n",
    "                will be determined by a same pruning ratio or the number given by a dict.\n",
    "                if set to \"global\", the number of redundant neurons in each layer will be determined according to the score.\n",
    "\n",
    "    The methods are:\n",
    "\n",
    "        add(layer_name, neuron_id, score): add an neuron in the neuron list.\n",
    "            params:\n",
    "                layer_name: str, the name of layer that contains the neuron\n",
    "                neuron_id: int, the index of the neuron in this layer\n",
    "                score: float, the contribution score of the layer\n",
    "\n",
    "        sort(): sort the neurons according the contribution score.\n",
    "        get_redundant_neurons(p): return redundant neurons\n",
    "            params:\n",
    "                p: float or dict, if dict, the mode must be layer_wise. if float(0.0~1.0), in layer_wise mode, the redundant neurons\n",
    "                   in each layer will be selected according to this parameter. In \"global\" mode, this is the overall pruning ratio\n",
    "                   of the whole network.\n",
    "        print_info(): print infos.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, protect_dict=None, mode=\"global\"):\n",
    "        self.netscores = {}\n",
    "        self.sorted = False\n",
    "        self.total_num = 0\n",
    "        self.protected_num = 0\n",
    "        self.neuron_num = {}\n",
    "        self.prune_num = {}\n",
    "        self.protect_dict = protect_dict\n",
    "        self.mode = mode\n",
    "        self.redundant = None\n",
    "\n",
    "    def add(self, layer_name, neuron_id, score):\n",
    "        if self.netscores.has_key(layer_name):\n",
    "            self.netscores[layer_name].append((layer_name, neuron_id, score))\n",
    "            self.neuron_num[layer_name] += 1\n",
    "        else:\n",
    "            self.netscores[layer_name] = [(layer_name, neuron_id, score)]\n",
    "            self.neuron_num[layer_name] = 1\n",
    "\n",
    "        self.total_num += 1\n",
    "\n",
    "    def sort(self):\n",
    "        if self.mode == \"layer_wise\":\n",
    "            for k in self.netscores.keys():\n",
    "                self.netscores[k] = sorted(self.netscores[k], key=lambda x:x[2])\n",
    "            self.sorted = True\n",
    "        elif self.mode == \"global\":\n",
    "            netscores = []\n",
    "            if self.protect_dict != None:\n",
    "                for k in self.netscores.keys():\n",
    "                    self.netscores[k] = sorted(self.netscores[k], key=lambda x:x[2])\n",
    "                    self.netscores[k] = self.netscores[k][:-self.protect_dict[k]]\n",
    "            for k in self.netscores.keys():\n",
    "                netscores += self.netscores[k]\n",
    "            self.netscores = sorted(netscores, key=lambda x:x[2])\n",
    "            self.protected_num = self.total_num - len(self.netscores)\n",
    "\n",
    "    def get_redundant_neurons(self, p):\n",
    "        if isinstance(p, float):\n",
    "            if self.mode == \"layer_wise\":\n",
    "                if not self.sorted:\n",
    "                    self.sort()\n",
    "                redundant = []\n",
    "                for k in self.netscores.keys():\n",
    "                    redundant += self.netscores[k][:np.round(self.neuron_num[k]*p)]\n",
    "\n",
    "                self.redundant = redundant\n",
    "            elif self.mode == \"global\":\n",
    "                self.redundant = self.netscores[:int(np.round(self.total_num*p))]\n",
    "\n",
    "        if isinstance(p, dict):\n",
    "            if self.mode == \"layer_wise\":\n",
    "                if not self.sorted:\n",
    "                    self.sort()\n",
    "                redundant = []\n",
    "\n",
    "                for k in self.netscores.keys():\n",
    "                    redundant += self.netscores[k][:p[k]]\n",
    "                self.redundant = redundant\n",
    "            elif self.mode == \"global\":\n",
    "                raise TypeError(\"dict only work for global mode!\")\n",
    "                    \n",
    "\n",
    "        return self.redundant\n",
    "\n",
    "    def print_info(self):\n",
    "        for neuron in self.redundant:\n",
    "            layer_name, neuron_id, score = neuron\n",
    "            if self.prune_num.has_key(layer_name):\n",
    "                self.prune_num[layer_name] += 1\n",
    "            else:\n",
    "                self.prune_num[layer_name] = 1\n",
    "        if self.protect_dict == None:\n",
    "            for k in self.prune_num.keys():\n",
    "                print(\"%s: pruned:%d, protected:%d\"%(k, self.prune_num[k], 0))\n",
    "        else:\n",
    "            for k in self.prune_num.keys():\n",
    "                print(\"%s: pruned:%d, protected:%d\"%(k, self.prune_num[k], self.protect_dict[k]))\n",
    "        \n",
    "        print(\"pruning: %d -> %d, %d get pruned\"%(self.total_num, self.total_num-len(self.redundant), len(self.redundant)))\n",
    "\n",
    "\n",
    "def get_num_by_ratio(neuron_num,p):\n",
    "    \"\"\"\n",
    "    this function returns the dictionary according to pruning ratio.\n",
    "    this function should use with mode=\"layer_wise\" and the case when p is a dict.\n",
    "    We use np.round to erase the errors introduced by integer division\n",
    "    \"\"\"\n",
    "    total_num = 0\n",
    "    for k in neuron_num:\n",
    "        total_num += neuron_num[k]\n",
    "    total_pruned_num = np.round(total_num*p)\n",
    "    for k in neuron_num:\n",
    "        neuron_num[k] = int(np.round(total_pruned_num*(neuron_num[k]/float(total_num))))\n",
    "    return neuron_num\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning....\n",
      "block1_conv2: pruned:2, protected:32\n",
      "fc2: pruned:145, protected:512\n",
      "block1_conv1: pruned:2, protected:32\n",
      "block2_conv2: pruned:1, protected:64\n",
      "block2_conv1: pruned:2, protected:64\n",
      "pruning: 10154 -> 10002, 152 get pruned\n",
      "Moving Network\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 510s - loss: 0.0366 - acc: 0.9969 - val_loss: 0.1392 - val_acc: 0.9860\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 507s - loss: 0.0301 - acc: 0.9978 - val_loss: 0.1310 - val_acc: 0.9878\n",
      "round:19 nb_neurons:10154\n",
      "\n",
      "------------------------------\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "block1_conv1 (Convolution2D)     (None, 224, 224, 59)  1652        convolution2d_input_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "block1_conv2 (Convolution2D)     (None, 224, 224, 58)  30856       block1_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block1_pool (MaxPooling2D)       (None, 112, 112, 58)  0           block1_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block2_conv1 (Convolution2D)     (None, 112, 112, 112) 58576       block1_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block2_conv2 (Convolution2D)     (None, 112, 112, 126) 127134      block2_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)       (None, 56, 56, 126)   0           block2_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv1 (Convolution2D)     (None, 56, 56, 256)   290560      block2_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv2 (Convolution2D)     (None, 56, 56, 256)   590080      block3_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv3 (Convolution2D)     (None, 56, 56, 256)   590080      block3_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)       (None, 28, 28, 256)   0           block3_conv3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block4_conv1 (Convolution2D)     (None, 28, 28, 512)   1180160     block3_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block4_conv2 (Convolution2D)     (None, 28, 28, 512)   2359808     block4_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block4_conv3 (Convolution2D)     (None, 28, 28, 512)   2359808     block4_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)       (None, 14, 14, 512)   0           block4_conv3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block5_conv1 (Convolution2D)     (None, 14, 14, 512)   2359808     block4_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block5_conv2 (Convolution2D)     (None, 14, 14, 512)   2359808     block5_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block5_conv3 (Convolution2D)     (None, 14, 14, 512)   2359808     block5_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block5_pool (MaxPooling2D)       (None, 7, 7, 512)     0           block5_conv3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "flatten (Flatten)                (None, 25088)         0           block5_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "fc1 (Dense)                      (None, 4096)          102764544   flatten[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "fc2 (Dense)                      (None, 1711)          7009967     fc1[0][0]                        \n",
      "____________________________________________________________________________________________________\n",
      "decision (Dense)                 (None, 2)             3424        fc2[0][0]                        \n",
      "====================================================================================================\n",
      "Total params: 124446073\n",
      "____________________________________________________________________________________________________\n",
      "Pruning....\n",
      "block1_conv2: pruned:1, protected:32\n",
      "fc2: pruned:141, protected:512\n",
      "block1_conv1: pruned:3, protected:32\n",
      "block2_conv1: pruned:5, protected:64\n",
      "pruning: 10002 -> 9852, 150 get pruned\n",
      "Moving Network\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[256,256,3,3]\n\t [[Node: Conv2D_45 = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Relu_50, block3_conv3_W_3/read)]]\n\t [[Node: Mean_15/_6110 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_1423_Mean_15\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\nCaused by op u'Conv2D_45', defined at:\n  File \"/home/gpu3/anaconda2/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/home/gpu3/anaconda2/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/traitlets/config/application.py\", line 592, in launch_instance\n    app.start()\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 403, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 151, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/tornado/ioloop.py\", line 866, in start\n    handler_func(fd_obj, events)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 433, in _handle_events\n    self._handle_recv()\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 465, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 407, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 260, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 212, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 370, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 175, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2902, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-2-0f35b7f44111>\", line 127, in <module>\n    model = MoveNetwork(model, \"block5_pool\")\n  File \"<ipython-input-1-a5c3c91046c8>\", line 106, in MoveNetwork\n    model.add(new_layer)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/keras/models.py\", line 312, in add\n    output_tensor = layer(self.outputs[0])\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/keras/engine/topology.py\", line 514, in __call__\n    self.add_inbound_node(inbound_layers, node_indices, tensor_indices)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/keras/engine/topology.py\", line 572, in add_inbound_node\n    Node.create_node(self, inbound_layers, node_indices, tensor_indices)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/keras/engine/topology.py\", line 149, in create_node\n    output_tensors = to_list(outbound_layer.call(input_tensors[0], mask=input_masks[0]))\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/keras/layers/convolutional.py\", line 466, in call\n    filter_shape=self.W_shape)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 1639, in conv2d\n    x = tf.nn.conv2d(x, kernel, strides, padding=padding)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 394, in conv2d\n    data_format=data_format, name=name)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2333, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1252, in __init__\n    self._traceback = _extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-0f35b7f44111>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    137\u001b[0m                      nesterov=True)\n\u001b[0;32m    138\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msgd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamples_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_val_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"vgg_catdog_\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprune_round\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\".h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/gpu3/anaconda2/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, **kwargs)\u001b[0m\n\u001b[0;32m    880\u001b[0m                                         \u001b[0mmax_q_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_q_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    881\u001b[0m                                         \u001b[0mnb_worker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnb_worker\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 882\u001b[1;33m                                         pickle_safe=pickle_safe)\n\u001b[0m\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m     def evaluate_generator(self, generator, val_samples,\n",
      "\u001b[1;32m/home/gpu3/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe)\u001b[0m\n\u001b[0;32m   1459\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   1460\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1461\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   1462\u001b[0m                 \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1463\u001b[0m                     \u001b[0m_stop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/gpu3/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1237\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1240\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/gpu3/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[0mupdated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/gpu3/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    715\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 717\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    718\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    719\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/gpu3/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    913\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 915\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    916\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/gpu3/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 965\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/home/gpu3/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m    983\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[256,256,3,3]\n\t [[Node: Conv2D_45 = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Relu_50, block3_conv3_W_3/read)]]\n\t [[Node: Mean_15/_6110 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_1423_Mean_15\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\nCaused by op u'Conv2D_45', defined at:\n  File \"/home/gpu3/anaconda2/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/home/gpu3/anaconda2/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/traitlets/config/application.py\", line 592, in launch_instance\n    app.start()\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 403, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 151, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/tornado/ioloop.py\", line 866, in start\n    handler_func(fd_obj, events)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 433, in _handle_events\n    self._handle_recv()\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 465, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 407, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 260, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 212, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 370, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 175, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2902, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-2-0f35b7f44111>\", line 127, in <module>\n    model = MoveNetwork(model, \"block5_pool\")\n  File \"<ipython-input-1-a5c3c91046c8>\", line 106, in MoveNetwork\n    model.add(new_layer)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/keras/models.py\", line 312, in add\n    output_tensor = layer(self.outputs[0])\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/keras/engine/topology.py\", line 514, in __call__\n    self.add_inbound_node(inbound_layers, node_indices, tensor_indices)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/keras/engine/topology.py\", line 572, in add_inbound_node\n    Node.create_node(self, inbound_layers, node_indices, tensor_indices)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/keras/engine/topology.py\", line 149, in create_node\n    output_tensors = to_list(outbound_layer.call(input_tensors[0], mask=input_masks[0]))\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/keras/layers/convolutional.py\", line 466, in call\n    filter_shape=self.W_shape)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 1639, in conv2d\n    x = tf.nn.conv2d(x, kernel, strides, padding=padding)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 394, in conv2d\n    data_format=data_format, name=name)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2333, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1252, in __init__\n    self._traceback = _extract_stack()\n"
     ]
    }
   ],
   "source": [
    "method = \"AAWS\"\n",
    "mode = \"global\"\n",
    "#pruning_ratio = 0.03\n",
    "for pruning_ratio in [0.015]:\n",
    "    prune_round = 19\n",
    "    target_layers = [\"block1_conv1\",\"block1_conv2\",\n",
    "                   \"block2_conv1\",\"block2_conv2\",\n",
    "                   \"block3_conv1\",\"block3_conv2\",\"block3_conv3\",\n",
    "                   \"block4_conv1\",\"block4_conv2\",\"block4_conv3\",\n",
    "                   \"block5_conv1\",\"block5_conv2\",\"block5_conv3\",\"fc1\", \"fc2\"] \n",
    "\n",
    "    while True:\n",
    "        model_name = \"vgg_catdog_\"+str(prune_round-1)+\".h5\"\n",
    "        model = load_model(model_name)\n",
    "        protect_dict = {\"block1_conv1\":32,\n",
    "                        \"block1_conv2\":32,\n",
    "                       \"block2_conv1\":64,\n",
    "                        \"block2_conv2\":64,\n",
    "                       \"block3_conv1\":128,\n",
    "                        \"block3_conv2\":128,\n",
    "                        \"block3_conv3\":128,\n",
    "                       \"block4_conv1\":256,\n",
    "                        \"block4_conv2\":256,\n",
    "                        \"block4_conv3\":256,\n",
    "                       \"block5_conv1\":256,\n",
    "                        \"block5_conv2\":256,\n",
    "                        \"block5_conv3\":256,\n",
    "                        \"fc1\":512, \n",
    "                       \"fc2\":512}\n",
    "        model.trainable = True\n",
    "        netscore = NetScore(mode=mode, protect_dict=protect_dict)\n",
    "\n",
    "        if method == 'AAWS':\n",
    "            for target_layer in target_layers:\n",
    "                if target_layer=='fc1':\n",
    "                    w,b = model.get_layer(\"fc2\").get_weights()\n",
    "                    weight_sum = np.mean(np.abs(w), axis=1)\n",
    "                    weight_sum = weight_sum/np.mean(weight_sum)\n",
    "                elif target_layer==\"fc2\":\n",
    "                    w,b = model.get_layer(\"decision\").get_weights()\n",
    "                    weight_sum = np.mean(np.abs(w), axis=1)\n",
    "                    weight_sum = weight_sum/np.mean(weight_sum)\n",
    "                else:\n",
    "                    w,b = model.get_layer(target_layer).get_weights()\n",
    "                    weight_sum = np.mean(np.abs(w), axis=(0,1,2))\n",
    "                    weight_sum = weight_sum/np.mean(weight_sum)\n",
    "\n",
    "                for neuron_id in range(weight_sum.shape[0]):\n",
    "                    netscore.add(target_layer, neuron_id, weight_sum[neuron_id])\n",
    "        elif method == \"mean\" or method == \"sigma\":\n",
    "            mean_dict = {}\n",
    "            std_dict = {}\n",
    "            all_data = {}\n",
    "            for target_layer in target_layers:\n",
    "\n",
    "                f = h5py.File(\"prune\"+str(prune_round)+'_'+target_layer+'_catdog.h5', 'r')\n",
    "                raw_stat = {}\n",
    "\n",
    "                for k in f.keys():\n",
    "                    raw_stat[k] = f[k][:]\n",
    "\n",
    "\n",
    "                all_data[target_layer] = raw_stat\n",
    "                # put activation together\n",
    "                act_data = []\n",
    "                for k in raw_stat.keys():\n",
    "                    act_data += list(raw_stat[k])\n",
    "\n",
    "                act_data = np.array(act_data)\n",
    "\n",
    "                mean_dict[target_layer] = np.mean(act_data, axis=0)\n",
    "                std_dict[target_layer] = np.std(act_data, axis=0)\n",
    "                mean_dict[target_layer] = mean_dict[target_layer]/np.mean(mean_dict[target_layer])\n",
    "                std_dict[target_layer] = std_dict[target_layer]/np.mean(std_dict[target_layer])\n",
    "            if method == 'sigma':\n",
    "                for target_layer in target_layers:\n",
    "                    for neuron_id in range(std_dict[target_layer].shape[0]):\n",
    "                        netscore.add(target_layer, neuron_id, std_dict[target_layer][neuron_id])\n",
    "\n",
    "            elif method == 'mean':\n",
    "                for target_layer in target_layers:\n",
    "                    for neuron_id in range(mean_dict[target_layer].shape[0]):\n",
    "                        netscore.add(target_layer, neuron_id, mean_dict[target_layer][neuron_id])\n",
    "                    \n",
    "        if mode=='layer_wise':\n",
    "            neuron_num = netscore.neuron_num\n",
    "            p = get_num_by_ratio(neuron_num, pruning_ratio)\n",
    "\n",
    "        else:\n",
    "            p = pruning_ratio\n",
    "            \n",
    "        netscore.sort()\n",
    "        redundant_neurons = netscore.get_redundant_neurons(p)\n",
    "\n",
    "        print(\"Pruning....\")\n",
    "        for neuron in redundant_neurons:\n",
    "            neuron_layer, neuron_id, neuron_score = neuron\n",
    "            if neuron_layer=='fc1':\n",
    "                w,b = model.get_layer(\"fc2\").get_weights()\n",
    "                w[neuron_id,:] = 0.\n",
    "                model.get_layer(\"fc2\").set_weights([w,b])\n",
    "\n",
    "                w,b = model.get_layer(neuron_layer).get_weights()\n",
    "                w[:, neuron_id] = 0.\n",
    "                b[neuron_id] = 0.\n",
    "                model.get_layer(neuron_layer).set_weights([w,b])\n",
    "\n",
    "            elif neuron_layer=='fc2':\n",
    "                w,b = model.get_layer(\"decision\").get_weights()\n",
    "                w[neuron_id,:] = 0.\n",
    "                model.get_layer(\"decision\").set_weights([w,b])\n",
    "\n",
    "                w,b = model.get_layer(neuron_layer).get_weights()\n",
    "                w[:, neuron_id] = 0.\n",
    "                b[neuron_id] = 0.\n",
    "                model.get_layer(neuron_layer).set_weights([w,b])\n",
    "            else:\n",
    "            # do pruning to target_layer\n",
    "                w,b = model.get_layer(neuron_layer).get_weights()\n",
    "                w[:,:,:,neuron_id] = 0.\n",
    "                b[neuron_id] = 0.\n",
    "                model.get_layer(neuron_layer).set_weights([w,b])\n",
    "\n",
    "\n",
    "        netscore.print_info()\n",
    "        print(\"Moving Network\")\n",
    "        model = MoveNetwork(model, \"block5_pool\")\n",
    "\n",
    "\n",
    "\n",
    "    # fine-tune\n",
    "        test_gen = data_gen('test.txt')\n",
    "        train_gen = data_gen(\"train.txt\")\n",
    "        \n",
    "\n",
    "        sgd = SGD(lr=0.00001, momentum=0.9, decay=5*1e-4,\n",
    "                     nesterov=True)\n",
    "        model.compile(loss = 'categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "        model.fit_generator(train_gen, samples_per_epoch=20000, nb_epoch=2, verbose=1, validation_data=test_gen, nb_val_samples=5000)\n",
    "\n",
    "        model.save(\"vgg_catdog_\"+str(prune_round)+\".h5\")\n",
    "\n",
    "        record = \"round:%d nb_neurons:%d\\n\"%(prune_round, netscore.total_num)\n",
    "        print(record)\n",
    "        print(\"------------------------------\")\n",
    "        prune_round+=1\n",
    "        model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# global\n",
    "\n",
    "target_layers =[\"block1_conv1\",\"block1_conv2\",\n",
    "               \"block2_conv1\",\"block2_conv2\",\n",
    "               \"block3_conv1\",\"block3_conv2\",\"block3_conv3\",\n",
    "               \"block4_conv195\",\"block4_conv2\",\"block4_conv3\",\n",
    "               \"block5_conv1\",\"block5_conv2\",\"block5_conv3\",\n",
    "               \"fc1\",\"fc2\"]\n",
    "\n",
    "mean_dict = {}\n",
    "std_dict = {}\n",
    "all_data = {}\n",
    "for target_layer in target_layers:\n",
    "\n",
    "    f = h5py.File(\"prune\"+str(1)+'_'+target_layer+'_catdog.h5', 'r')\n",
    "    raw_stat = {}\n",
    "\n",
    "    for k in f.keys():\n",
    "        raw_stat[k] = f[k][:]\n",
    "\n",
    "\n",
    "    all_data[target_layer] = raw_stat\n",
    "    # put activation together\n",
    "    act_data = []\n",
    "    for k in raw_stat.keys():\n",
    "        act_data += list(raw_stat[k])\n",
    "\n",
    "    act_data = np.array(act_data)\n",
    "\n",
    "    mean_dict[target_layer] = np.mean(act_data, axis=0)\n",
    "    std_dict[target_layer] = np.std(act_data, axis=0)\n",
    "    mean_dict[target_layer] = mean_dict[target_layer]/np.mean(mean_dict[target_layer])\n",
    "    std_dict[target_layer] = std_dict[target_layer]/np.mean(std_dict[target_layer])\n",
    "    \n",
    "for method in ['min_sigma', 'min_mean']:\n",
    "    model = load_model(\"vgg_catdog_0.h5\")\n",
    "    g = open('global_'+method+'catdog', 'w')\n",
    "    records= []\n",
    "    netscore = NetScore()\n",
    "    if method == 'AWS':\n",
    "        for target_layer in target_layers:\n",
    "            if target_layer[:2]=='fc':\n",
    "                w,b = model.get_layer(next_layer(target_layer)).get_weights()\n",
    "                weight_sum = np.mean(np.abs(w), axis=1)\n",
    "                weight_sum = weight_sum/np.mean(weight_sum)\n",
    "            else:\n",
    "                w,b = model.get_layer(target_layer).get_weights()\n",
    "                weight_sum = np.mean(np.abs(w), axis=(0,1,2))\n",
    "                weight_sum = weight_sum/np.mean(weight_sum)\n",
    "            for neuron_id in range(weight_sum.shape[0]):\n",
    "                netscore.add(target_layer, neuron_id, weight_sum[neuron_id])\n",
    "    elif method == 'min_sigma':\n",
    "        for target_layer in target_layers:\n",
    "            for neuron_id in range(std_dict[target_layer].shape[0]):\n",
    "                netscore.add(target_layer, neuron_id, std_dict[target_layer][neuron_id])\n",
    "\n",
    "    elif method == 'min_mean':\n",
    "        for target_layer in target_layers:\n",
    "            for neuron_id in range(mean_dict[target_layer].shape[0]):\n",
    "                netscore.add(target_layer, neuron_id, mean_dict[target_layer][neuron_id])\n",
    "    \n",
    "    netscore.rank()\n",
    "    \n",
    "    for pruning_ratio in np.arange(0.01,0.16,0.01):\n",
    "        redundant_neurons = netscore.get_redundant_neurons(pruning_ratio)\n",
    "        for neuron in redundant_neurons:\n",
    "            neuron_layer, neuron_id, neuron_score = neuron\n",
    "            if neuron_layer[:2]=='fc':\n",
    "                w,b = model.get_layer(next_layer(neuron_layer)).get_weights()  \n",
    "                if method==\"min_sigma\":\n",
    "                    b += mean_dict[neuron_layer][neuron_id]*w[neuron_id,:]\n",
    "                w[neuron_id,:] = 0.\n",
    "                model.get_layer(next_layer(neuron_layer)).set_weights([w,b]) \n",
    "\n",
    "                w,b = model.get_layer(neuron_layer).get_weights()  \n",
    "                w[:, neuron_id] = 0.\n",
    "                b[neuron_id] = 0.\n",
    "                model.get_layer(neuron_layer).set_weights([w,b])\n",
    "\n",
    "            else:\n",
    "            # do pruning to target_layer\n",
    "                w,b = model.get_layer(neuron_layer).get_weights()      \n",
    "                w[:,:,:,neuron_id] = 0.\n",
    "                b[neuron_id] = 0.\n",
    "                model.get_layer(neuron_layer).set_weights([w,b])   \n",
    "\n",
    "        test_gen = data_gen('test.txt')\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "        score = model.evaluate_generator(test_gen, val_samples=5000)\n",
    "        records.append(str(pruning_ratio)+' '+str(score[1])+'\\n')\n",
    "        netscore.print_info(pruning_ratio)\n",
    "        print(\"------------------------------\")\n",
    "\n",
    "    g.writelines(records)\n",
    "    g.close()\n",
    "    print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing block1_conv1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data_gen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a184f892029e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mraw_stat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mtrain_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_gen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_gen' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import keras.backend as K\n",
    "import h5py\n",
    "\n",
    "\n",
    "prune_round = 4\n",
    "model_name = \"vgg_catdog_\"+str(prune_round-1)+\".h5\"\n",
    "model = load_model(model_name)\n",
    "\n",
    "\n",
    "target_layers =[\"block1_conv1\",\"block1_conv2\",\n",
    "               \"block2_conv1\",\"block2_conv2\",\n",
    "               \"block3_conv1\",\"block3_conv2\",\"block3_conv3\",\n",
    "               \"block4_conv1\",\"block4_conv2\",\"block4_conv3\",\n",
    "               \"block5_conv1\",\"block5_conv2\",\"block5_conv3\",\n",
    "               \"fc1\",\"fc2\"]\n",
    "\n",
    "for target_layer in target_layers:\n",
    "    print(\"processing \"+target_layer)\n",
    "\n",
    "    input_tensor = model.input\n",
    "    output_tensor = model.get_layer(target_layer).output\n",
    "\n",
    "    get_outputs = K.function([input_tensor, K.learning_phase()],\n",
    "                          [output_tensor])\n",
    "\n",
    "    raw_stat = {}\n",
    "    count = 0\n",
    "    train_gen = data_gen('train.txt')\n",
    "    for batch in train_gen:\n",
    "        count += 1\n",
    "        if count%50 == 0:\n",
    "            print(\"processing the %dth batch training sample\"%count)\n",
    "        imgs, label = batch\n",
    "        label = categorical_to_scalar(label)\n",
    "        output = get_outputs([imgs, 0])[0]\n",
    "        for i in range(output.shape[0]):\n",
    "            if len(output[i].shape) == 3:\n",
    "                output_sum = np.sum(output[i], axis = (0,1))\n",
    "            else:\n",
    "                assert(len(output[i].shape)==1)\n",
    "                output_sum = output[i]\n",
    "            \n",
    "            if raw_stat.has_key(label[i]):\n",
    "                raw_stat[label[i]].append(output_sum)\n",
    "            else:\n",
    "                raw_stat[label[i]] = [output_sum]\n",
    "        if count==500:\n",
    "            break\n",
    "\n",
    "        \n",
    "    f = h5py.File(\"prune\"+str(prune_round)+'_'+target_layer+'_catdog.h5','w')\n",
    "    for k in raw_stat.keys():\n",
    "        f.create_dataset(name=str(k), data=raw_stat[k])\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning....\n",
      "pruning: 12416 -> 12416, 0 get pruned\n",
      "method:AAWS ratio:0.000000 nb_neurons:12416 acc:0.982400\n",
      "\n",
      "------------------------------\n",
      "Pruning....\n",
      "fc2: pruned:247, protected:512\n",
      "block2_conv1: pruned:1, protected:64\n",
      "pruning: 12416 -> 12168, 248 get pruned\n",
      "method:AAWS ratio:0.020000 nb_neurons:12416 acc:0.982400\n",
      "\n",
      "------------------------------\n",
      "Pruning....\n",
      "fc2: pruned:495, protected:512\n",
      "block2_conv1: pruned:2, protected:64\n",
      "pruning: 12416 -> 11919, 497 get pruned\n",
      "method:AAWS ratio:0.040000 nb_neurons:12416 acc:0.982400\n",
      "\n",
      "------------------------------\n",
      "Pruning....\n",
      "fc2: pruned:742, protected:512\n",
      "block2_conv1: pruned:3, protected:64\n",
      "pruning: 12416 -> 11671, 745 get pruned\n",
      "method:AAWS ratio:0.060000 nb_neurons:12416 acc:0.982200\n",
      "\n",
      "------------------------------\n",
      "Pruning....\n",
      "fc2: pruned:1236, protected:512\n",
      "block1_conv1: pruned:2, protected:32\n",
      "block2_conv1: pruned:4, protected:64\n",
      "pruning: 12416 -> 11174, 1242 get pruned\n",
      "method:AAWS ratio:0.100000 nb_neurons:12416 acc:0.982400\n",
      "\n",
      "------------------------------\n",
      "Pruning....\n",
      "block1_conv2: pruned:3, protected:32\n",
      "fc2: pruned:1478, protected:512\n",
      "block1_conv1: pruned:2, protected:32\n",
      "block2_conv1: pruned:7, protected:64\n",
      "pruning: 12416 -> 10926, 1490 get pruned\n",
      "method:AAWS ratio:0.120000 nb_neurons:12416 acc:0.981800\n",
      "\n",
      "------------------------------\n",
      "Pruning....\n",
      "block1_conv2: pruned:7, protected:32\n",
      "fc2: pruned:1704, protected:512\n",
      "block1_conv1: pruned:8, protected:32\n",
      "block2_conv1: pruned:17, protected:64\n",
      "block2_conv2: pruned:2, protected:64\n",
      "pruning: 12416 -> 10678, 1738 get pruned\n",
      "method:AAWS ratio:0.140000 nb_neurons:12416 acc:0.975000\n",
      "\n",
      "------------------------------\n",
      "Pruning....\n",
      "fc2: pruned:1918, protected:512\n",
      "block2_conv1: pruned:27, protected:64\n",
      "block2_conv2: pruned:5, protected:64\n",
      "block5_conv3: pruned:2, protected:256\n",
      "block1_conv2: pruned:11, protected:32\n",
      "block1_conv1: pruned:17, protected:32\n",
      "block3_conv1: pruned:4, protected:128\n",
      "block3_conv3: pruned:1, protected:128\n",
      "block3_conv2: pruned:2, protected:128\n",
      "pruning: 12416 -> 10429, 1987 get pruned\n",
      "method:AAWS ratio:0.160000 nb_neurons:12416 acc:0.965600\n",
      "\n",
      "------------------------------\n",
      "Pruning....\n",
      "fc2: pruned:2079, protected:512\n",
      "block2_conv1: pruned:40, protected:64\n",
      "block2_conv2: pruned:17, protected:64\n",
      "block4_conv2: pruned:2, protected:256\n",
      "block4_conv1: pruned:3, protected:256\n",
      "block5_conv3: pruned:10, protected:256\n",
      "block5_conv2: pruned:1, protected:256\n",
      "block1_conv2: pruned:16, protected:32\n",
      "block1_conv1: pruned:25, protected:32\n",
      "block3_conv1: pruned:20, protected:128\n",
      "block3_conv3: pruned:13, protected:128\n",
      "block3_conv2: pruned:9, protected:128\n",
      "pruning: 12416 -> 10181, 2235 get pruned\n",
      "method:AAWS ratio:0.180000 nb_neurons:12416 acc:0.959400\n",
      "\n",
      "------------------------------\n",
      "Pruning....\n",
      "fc2: pruned:2176, protected:512\n",
      "block2_conv1: pruned:47, protected:64\n",
      "block2_conv2: pruned:27, protected:64\n",
      "block4_conv2: pruned:12, protected:256\n",
      "block4_conv3: pruned:12, protected:256\n",
      "block4_conv1: pruned:25, protected:256\n",
      "block5_conv3: pruned:26, protected:256\n",
      "block5_conv2: pruned:2, protected:256\n",
      "block5_conv1: pruned:1, protected:256\n",
      "block1_conv2: pruned:20, protected:32\n",
      "block1_conv1: pruned:28, protected:32\n",
      "block3_conv1: pruned:41, protected:128\n",
      "block3_conv3: pruned:22, protected:128\n",
      "block3_conv2: pruned:44, protected:128\n",
      "pruning: 12416 -> 9933, 2483 get pruned\n",
      "method:AAWS ratio:0.200000 nb_neurons:12416 acc:0.904800\n",
      "\n",
      "------------------------------\n",
      "Pruning....\n",
      "fc1: pruned:3, protected:512\n",
      "fc2: pruned:2237, protected:512\n",
      "block2_conv1: pruned:53, protected:64\n",
      "block2_conv2: pruned:38, protected:64\n",
      "block4_conv2: pruned:34, protected:256\n",
      "block4_conv3: pruned:31, protected:256\n",
      "block4_conv1: pruned:55, protected:256\n",
      "block5_conv3: pruned:52, protected:256\n",
      "block5_conv2: pruned:11, protected:256\n",
      "block5_conv1: pruned:6, protected:256\n",
      "block1_conv2: pruned:24, protected:32\n",
      "block1_conv1: pruned:30, protected:32\n",
      "block3_conv1: pruned:59, protected:128\n",
      "block3_conv3: pruned:41, protected:128\n",
      "block3_conv2: pruned:58, protected:128\n",
      "pruning: 12416 -> 9684, 2732 get pruned\n",
      "method:AAWS ratio:0.220000 nb_neurons:12416 acc:0.776800\n",
      "\n",
      "------------------------------\n",
      "Pruning....\n",
      "pruning: 12416 -> 12416, 0 get pruned\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[20,64,224,224]\n\t [[Node: Conv2D_144 = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Relu_165, block1_conv2_W_11/read)]]\nCaused by op u'Conv2D_144', defined at:\n  File \"/home/gpu3/anaconda2/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/home/gpu3/anaconda2/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/traitlets/config/application.py\", line 592, in launch_instance\n    app.start()\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 403, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 151, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/tornado/ioloop.py\", line 866, in start\n    handler_func(fd_obj, events)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 433, in _handle_events\n    self._handle_recv()\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 465, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 407, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 260, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 212, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 370, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 175, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2902, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-2-6c073dfd81a5>\", line 29, in <module>\n    model = load_model(model_name)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/keras/models.py\", line 128, in load_model\n    model = model_from_config(model_config, custom_objects=custom_objects)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/keras/models.py\", line 177, in model_from_config\n    return layer_from_config(config, custom_objects=custom_objects)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/keras/utils/layer_utils.py\", line 36, in layer_from_config\n    return layer_class.from_config(config['config'])\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/keras/engine/topology.py\", line 2375, in from_config\n    process_layer(layer_data)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/keras/engine/topology.py\", line 2370, in process_layer\n    layer(input_tensors[0])\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/keras/engine/topology.py\", line 514, in __call__\n    self.add_inbound_node(inbound_layers, node_indices, tensor_indices)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/keras/engine/topology.py\", line 572, in add_inbound_node\n    Node.create_node(self, inbound_layers, node_indices, tensor_indices)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/keras/engine/topology.py\", line 149, in create_node\n    output_tensors = to_list(outbound_layer.call(input_tensors[0], mask=input_masks[0]))\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/keras/layers/convolutional.py\", line 466, in call\n    filter_shape=self.W_shape)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 1639, in conv2d\n    x = tf.nn.conv2d(x, kernel, strides, padding=padding)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 394, in conv2d\n    data_format=data_format, name=name)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2333, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1252, in __init__\n    self._traceback = _extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-6c073dfd81a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    136\u001b[0m                      nesterov=True)\n\u001b[0;32m    137\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msgd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/gpu3/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mevaluate_generator\u001b[1;34m(self, generator, val_samples, max_q_size, nb_worker, pickle_safe)\u001b[0m\n\u001b[0;32m   1576\u001b[0m                                 'or (x, y). Found: ' + str(generator_output))\n\u001b[0;32m   1577\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1578\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1579\u001b[0m             \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1580\u001b[0m                 \u001b[0m_stop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/gpu3/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtest_on_batch\u001b[1;34m(self, x, y, sample_weight)\u001b[0m\n\u001b[0;32m   1275\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1276\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1277\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1278\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1279\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/gpu3/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[0mupdated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/gpu3/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    715\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 717\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    718\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    719\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/gpu3/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    913\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 915\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    916\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/gpu3/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 965\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/home/gpu3/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m    983\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[20,64,224,224]\n\t [[Node: Conv2D_144 = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Relu_165, block1_conv2_W_11/read)]]\nCaused by op u'Conv2D_144', defined at:\n  File \"/home/gpu3/anaconda2/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/home/gpu3/anaconda2/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/traitlets/config/application.py\", line 592, in launch_instance\n    app.start()\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 403, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 151, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/tornado/ioloop.py\", line 866, in start\n    handler_func(fd_obj, events)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 433, in _handle_events\n    self._handle_recv()\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 465, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 407, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 260, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 212, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 370, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 175, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2902, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-2-6c073dfd81a5>\", line 29, in <module>\n    model = load_model(model_name)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/keras/models.py\", line 128, in load_model\n    model = model_from_config(model_config, custom_objects=custom_objects)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/keras/models.py\", line 177, in model_from_config\n    return layer_from_config(config, custom_objects=custom_objects)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/keras/utils/layer_utils.py\", line 36, in layer_from_config\n    return layer_class.from_config(config['config'])\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/keras/engine/topology.py\", line 2375, in from_config\n    process_layer(layer_data)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/keras/engine/topology.py\", line 2370, in process_layer\n    layer(input_tensors[0])\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/keras/engine/topology.py\", line 514, in __call__\n    self.add_inbound_node(inbound_layers, node_indices, tensor_indices)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/keras/engine/topology.py\", line 572, in add_inbound_node\n    Node.create_node(self, inbound_layers, node_indices, tensor_indices)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/keras/engine/topology.py\", line 149, in create_node\n    output_tensors = to_list(outbound_layer.call(input_tensors[0], mask=input_masks[0]))\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/keras/layers/convolutional.py\", line 466, in call\n    filter_shape=self.W_shape)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 1639, in conv2d\n    x = tf.nn.conv2d(x, kernel, strides, padding=padding)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 394, in conv2d\n    data_format=data_format, name=name)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2333, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/gpu3/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1252, in __init__\n    self._traceback = _extract_stack()\n"
     ]
    }
   ],
   "source": [
    "## no fine-tune\n",
    "\n",
    "mode = \"global\"\n",
    "#pruning_ratio = 0.03\n",
    "\n",
    "for method in [\"AAWS\", \"mean\"]:\n",
    "    for pruning_ratio in [0., 0.02, 0.04, 0.06, 0.1, 0.12,0.14,0.16,0.18,0.2,0.22]:\n",
    "        prune_round = 1\n",
    "        target_layers = [\"block1_conv1\",\"block1_conv2\",\n",
    "                   \"block2_conv1\",\"block2_conv2\",\n",
    "                   \"block3_conv1\",\"block3_conv2\",\"block3_conv3\",\n",
    "                   \"block4_conv1\",\"block4_conv2\",\"block4_conv3\",\n",
    "                   \"block5_conv1\",\"block5_conv2\",\"block5_conv3\",\"fc1\", \"fc2\"] \n",
    "\n",
    "        protect_dict = {\"block1_conv1\":32,\n",
    "                        \"block1_conv2\":32,\n",
    "                       \"block2_conv1\":64,\n",
    "                        \"block2_conv2\":64,\n",
    "                       \"block3_conv1\":128,\n",
    "                        \"block3_conv2\":128,\n",
    "                        \"block3_conv3\":128,\n",
    "                       \"block4_conv1\":256,\n",
    "                        \"block4_conv2\":256,\n",
    "                        \"block4_conv3\":256,\n",
    "                       \"block5_conv1\":256,\n",
    "                        \"block5_conv2\":256,\n",
    "                        \"block5_conv3\":256,\n",
    "                        \"fc1\":512, \n",
    "                        \"fc2\":512}\n",
    "        model_name = \"vgg_catdog_\"+str(prune_round-1)+\".h5\"\n",
    "        model = load_model(model_name)\n",
    "        \n",
    "        model.trainable = True\n",
    "        netscore = NetScore(mode=mode, protect_dict=protect_dict)\n",
    "\n",
    "        if method == 'AAWS':\n",
    "            for target_layer in target_layers:\n",
    "                if target_layer=='fc1':\n",
    "                    w,b = model.get_layer(\"fc2\").get_weights()\n",
    "                    weight_sum = np.mean(np.abs(w), axis=1)\n",
    "                    weight_sum = weight_sum/np.mean(weight_sum)\n",
    "                elif target_layer==\"fc2\":\n",
    "                    w,b = model.get_layer(\"decision\").get_weights()\n",
    "                    weight_sum = np.mean(np.abs(w), axis=1)\n",
    "                    weight_sum = weight_sum/np.mean(weight_sum)\n",
    "                else:\n",
    "                    w,b = model.get_layer(target_layer).get_weights()\n",
    "                    weight_sum = np.mean(np.abs(w), axis=(0,1,2))\n",
    "                    weight_sum = weight_sum/np.mean(weight_sum)\n",
    "\n",
    "                for neuron_id in range(weight_sum.shape[0]):\n",
    "                    netscore.add(target_layer, neuron_id, weight_sum[neuron_id])\n",
    "        elif method == \"mean\" or method == \"sigma\":\n",
    "            mean_dict = {}\n",
    "            std_dict = {}\n",
    "            all_data = {}\n",
    "            for target_layer in target_layers:\n",
    "\n",
    "                f = h5py.File(\"prune\"+str(prune_round)+'_'+target_layer+'_catdog.h5', 'r')\n",
    "                raw_stat = {}\n",
    "\n",
    "                for k in f.keys():\n",
    "                    raw_stat[k] = f[k][:]\n",
    "\n",
    "\n",
    "                all_data[target_layer] = raw_stat\n",
    "                # put activation together\n",
    "                act_data = []\n",
    "                for k in raw_stat.keys():\n",
    "                    act_data += list(raw_stat[k])\n",
    "\n",
    "                act_data = np.array(act_data)\n",
    "\n",
    "                mean_dict[target_layer] = np.mean(act_data, axis=0)\n",
    "                std_dict[target_layer] = np.std(act_data, axis=0)\n",
    "                mean_dict[target_layer] = mean_dict[target_layer]/np.mean(mean_dict[target_layer])\n",
    "                std_dict[target_layer] = std_dict[target_layer]/np.mean(std_dict[target_layer])\n",
    "            if method == 'sigma':\n",
    "                for target_layer in target_layers:\n",
    "                    for neuron_id in range(std_dict[target_layer].shape[0]):\n",
    "                        netscore.add(target_layer, neuron_id, std_dict[target_layer][neuron_id])\n",
    "\n",
    "            elif method == 'mean':\n",
    "                for target_layer in target_layers:\n",
    "                    for neuron_id in range(mean_dict[target_layer].shape[0]):\n",
    "                        netscore.add(target_layer, neuron_id, mean_dict[target_layer][neuron_id])\n",
    "                    \n",
    "        if mode=='layer_wise':\n",
    "            neuron_num = netscore.neuron_num\n",
    "            p = get_num_by_ratio(neuron_num, pruning_ratio)\n",
    "\n",
    "        else:\n",
    "            p = pruning_ratio\n",
    "            \n",
    "        netscore.sort()\n",
    "        redundant_neurons = netscore.get_redundant_neurons(p)\n",
    "\n",
    "        print(\"Pruning....\")\n",
    "        for neuron in redundant_neurons:\n",
    "            neuron_layer, neuron_id, neuron_score = neuron\n",
    "            if neuron_layer=='fc1':\n",
    "                w,b = model.get_layer(\"fc2\").get_weights()\n",
    "                w[neuron_id,:] = 0.\n",
    "                model.get_layer(\"fc2\").set_weights([w,b])\n",
    "\n",
    "                w,b = model.get_layer(neuron_layer).get_weights()\n",
    "                w[:, neuron_id] = 0.\n",
    "                b[neuron_id] = 0.\n",
    "                model.get_layer(neuron_layer).set_weights([w,b])\n",
    "\n",
    "            elif neuron_layer=='fc2':\n",
    "                w,b = model.get_layer(\"decision\").get_weights()\n",
    "                w[neuron_id,:] = 0.\n",
    "                model.get_layer(\"decision\").set_weights([w,b])\n",
    "\n",
    "                w,b = model.get_layer(neuron_layer).get_weights()\n",
    "                w[:, neuron_id] = 0.\n",
    "                b[neuron_id] = 0.\n",
    "                model.get_layer(neuron_layer).set_weights([w,b])\n",
    "            else:\n",
    "            # do pruning to target_layer\n",
    "                w,b = model.get_layer(neuron_layer).get_weights()\n",
    "                w[:,:,:,neuron_id] = 0.\n",
    "                b[neuron_id] = 0.\n",
    "                model.get_layer(neuron_layer).set_weights([w,b])\n",
    "\n",
    "\n",
    "        netscore.print_info()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # fine-tune\n",
    "        test_gen = data_gen('test.txt')\n",
    "        \n",
    "\n",
    "        sgd = SGD(lr=0.00001, momentum=0.9, decay=5*1e-4,\n",
    "                     nesterov=True)\n",
    "        model.compile(loss = 'categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "        score = model.evaluate_generator(test_gen, 5000)\n",
    "\n",
    "\n",
    "        record = \"method:%s ratio:%f nb_neurons:%d acc:%f\\n\"%(method, pruning_ratio, netscore.total_num, score[1])\n",
    "        print(record)\n",
    "        print(\"------------------------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
